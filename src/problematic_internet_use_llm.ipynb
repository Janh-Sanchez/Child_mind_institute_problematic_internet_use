{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b738c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "de19fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define robust directory paths within the notebook environment.\n",
    "BASE_DIR = Path().resolve()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "\n",
    "train_path = DATA_DIR / 'train.csv'\n",
    "test_path = DATA_DIR / 'test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a961ffb",
   "metadata": {},
   "source": [
    "Importacion de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5594db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "'# Load environment variables from .env file'\n",
    "def load_data(data_dir='data'):\n",
    "    try:\n",
    "        # Define robust paths for CSV and Parquet files\n",
    "        train_csv_path = os.path.join(data_dir, 'train.csv')\n",
    "        test_csv_path = os.path.join(data_dir, 'test.csv')\n",
    "        train_parquet_path = os.path.join(data_dir, 'train.parquet')\n",
    "        test_parquet_path = os.path.join(data_dir, 'test.parquet')\n",
    "        \n",
    "        if not all(os.path.exists(f) for f in [train_csv_path, test_csv_path]):\n",
    "            raise FileNotFoundError(\"CSV files not found in the ‚Äòdata‚Äô folder.\")\n",
    "        \n",
    "        train_csv = pd.read_csv(train_csv_path)\n",
    "        test_csv = pd.read_csv(test_csv_path)\n",
    "        \n",
    "        train_parquet = pd.DataFrame()\n",
    "        test_parquet = pd.DataFrame()\n",
    "        if os.path.exists(train_parquet_path):\n",
    "            train_parquet = pq.read_table(train_parquet_path).to_pandas()\n",
    "        if os.path.exists(test_parquet_path):\n",
    "            test_parquet = pq.read_table(test_parquet_path).to_pandas()\n",
    "        \n",
    "        # Merge CSV and Parquet data on 'Subject_ID'\n",
    "        train = pd.merge(train_csv, train_parquet, on='Subject_ID', how='left') if not train_parquet.empty else train_csv\n",
    "        test = pd.merge(test_csv, test_parquet, on='Subject_ID', how='left') if not test_parquet.empty else test_csv\n",
    "        \n",
    "        return train, test, {}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf01c1",
   "metadata": {},
   "source": [
    "LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "57b5ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # Load environment variables from .env file\n",
    "\n",
    "api_key = os.getenv(\"api1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4acffd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"api1\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")# Initialize OpenAI client with API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "efc004d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'# Function to query DeepSeek model with a prompt'\n",
    "def query_deepseek(prompt: str, model: str = \"deepseek/deepseek-r1:free\") -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\":\"user\", \"content\":prompt}],\n",
    "        temperature=0.6,\n",
    "        max_tokens=4096,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "76e22533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells found: 1\n"
     ]
    }
   ],
   "source": [
    "# Load the notebook loader from langchain_community\n",
    "from langchain_community.document_loaders import NotebookLoader\n",
    "\n",
    "loader = NotebookLoader(\"problematic_internet_use_llm.ipynb\", include_outputs=False, remove_newline=True)\n",
    "docs = loader.load()\n",
    "print(f\"Cells found: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a03c557",
   "metadata": {},
   "source": [
    "Prompts que hay que realizar de manera general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ee2d5864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "# Import the NotebookLoader from langchain_community.document_loaders\n",
      "from langchain_community.document_loaders import NotebookLoader\n",
      "\n",
      "# Initialize the NotebookLoader with the specified notebook file ('prueba-llm.ipynb')\n",
      "# - include_outputs=False: Exclude notebook cell outputs from loading\n",
      "# - remove_newline=True: Remove newline characters for cleaner text processing\n",
      "loader = NotebookLoader(\n",
      "    file_path=\"prueba-llm.ipynb\",\n",
      "    include_outputs=False,\n",
      "    remove_newline=True\n",
      ")\n",
      "\n",
      "# Load the notebook content into documents\n",
      "docs = loader.load()\n",
      "\n",
      "# Print the number of cells found in the notebook\n",
      "print(f\"Cells found: {len(docs)}\")\n",
      "\n",
      "# Changes made:\n",
      "# 1. Added clear comments explaining each section of the code\n",
      "# 2. Formatted the NotebookLoader initialization with line breaks for better readability\n",
      "# 3. Used the parameter name 'file_path' explicitly for clarity (though NotebookLoader accepts positional args)\n",
      "# 4. Maintained all original functionality while improving code style\n",
      "# 5. Kept the final print statement unchanged as it serves a clear diagnostic purpose\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "original_code = \"\"\"\n",
    "# Load the active notebook (you can see its name in Kaggle)\n",
    "from langchain_community.document_loaders import NotebookLoader\n",
    "\n",
    "loader = NotebookLoader(\"prueba-llm.ipynb\", include_outputs=False, remove_newline=True)\n",
    "docs = loader.load()\n",
    "print(f\"Cells found: {len(docs)}\")\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Rewrite the following code with style and efficiency improvements, without changing its functionality:\\n{original_code},\n",
    "also, everything that is not related to the code should be commented, including the changes made\"\"\"\n",
    "\n",
    "response = query_deepseek(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8a36b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'# Function to analyze a file and answer a question using DeepSeek'\n",
    "def analyze_file(filepath: str, question: str):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    prompt = f\"\"\"I have this file named {filepath} with the following content:\n",
    "\n",
    "[START OF FILE]\n",
    "{content}\n",
    "[END OF FILE]\n",
    "\n",
    "Now, please answer the following:\n",
    "{question}\n",
    "\"\"\"\n",
    "    response = query_deepseek(prompt)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2612cb40",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'src/data/series_test.parquet/id=001f3379/part-0.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43manalyze_file\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msrc/data/series_test.parquet/id=001f3379/part-0.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCan you review this file and recommend feature engineering steps?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36manalyze_file\u001b[39m\u001b[34m(filepath, question)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manalyze_file\u001b[39m(filepath: \u001b[38;5;28mstr\u001b[39m, question: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      4\u001b[39m         content = f.read()\n\u001b[32m      6\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mI have this file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with the following content:\u001b[39m\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[33m[START OF FILE]\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\TrabajoFinal\\Child_mind_institute_problematic_internet_use\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'src/data/series_test.parquet/id=001f3379/part-0.parquet'"
     ]
    }
   ],
   "source": [
    "analyze_file(\"data/data_dictionary.csv\", \"Can you review this file and recommend feature engineering steps?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1ba5802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para trabajar con archivos **Parquet** (un formato de almacenamiento columnar eficiente) y tomar decisiones sobre los datos, sigue estos pasos:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. **Visualizar datos de un archivo Parquet**\n",
      "#### Herramientas y m√©todos:\n",
      "\n",
      "**a. Usando Python (Pandas/PyArrow):**\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Leer el archivo Parquet\n",
      "df = pd.read_parquet(\"ruta/al/archivo.parquet\")\n",
      "\n",
      "# Ver las primeras filas\n",
      "print(df.head())\n",
      "\n",
      "# Ver estad√≠sticas descriptivas\n",
      "print(df.describe())\n",
      "\n",
      "# Ver estructura del DataFrame (columnas y tipos de datos)\n",
      "print(df.info())\n",
      "```\n",
      "\n",
      "**b. Con herramientas gr√°ficas:**\n",
      "- **DuckDB**: Ejecuta consultas SQL directamente sobre Parquet.\n",
      "  ```sql\n",
      "  SELECT * FROM 'datos.parquet' LIMIT 10;\n",
      "  ```\n",
      "- **Apache Arrow (PyArrow)**: Para inspeccionar el esquema:\n",
      "  ```python\n",
      "  import pyarrow.parquet as pq\n",
      "\n",
      "  tabla = pq.read_table(\"datos.parquet\")\n",
      "  print(tabla.schema)\n",
      "  ```\n",
      "\n",
      "**c. Herramientas externas:**\n",
      "- **Parquet Viewer** (Windows/Mac): Interfaz gr√°fica para abrir Parquet.\n",
      "- **VS Code Extension**: Extensiones como *Parquet Viewer* para previsualizaci√≥n directa.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Decidir qu√© datos manejar**\n",
      "#### Pasos para evaluar los datos:\n",
      "1. **Entender el esquema**:\n",
      "   - Identifica las columnas, tipos de datos y relaciones.\n",
      "   - Ejemplo: `df.columns` o `tabla.schema` en PyArrow.\n",
      "\n",
      "2. **Calidad de datos**:\n",
      "   - Busca valores nulos: `df.isnull().sum()`.\n",
      "   - Detecta duplicados: `df.duplicated().sum()`.\n",
      "   - Verifica valores at√≠picos: Gr√°ficos (boxplots) o estad√≠sticas (percentiles).\n",
      "\n",
      "3. **Relevancia para el objetivo**:\n",
      "   - Elimina columnas irrelevantes (ej: IDs, metadatos innecesarios).\n",
      "   - Ejemplo: `df.drop(columns=[\"columna_innecesaria\"], inplace=True)`.\n",
      "\n",
      "4. **Distribuci√≥n de los datos**:\n",
      "   - Usa histogramas o `sns.pairplot()` para entender patrones.\n",
      "   - Agrupa datos: `df.groupby(\"categor√≠a\").mean()`.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Mejorar los datos**\n",
      "#### T√©cnicas comunes:\n",
      "- **Limpieza**:\n",
      "  - Eliminar filas con nulos: `df.dropna()`.\n",
      "  - Imputar valores faltantes: `df[\"columna\"].fillna(valor)`.\n",
      "  - Corregir formatos (ej: fechas mal escritas).\n",
      "\n",
      "- **Transformaci√≥n**:\n",
      "  - Normalizar/escalar: `from sklearn.preprocessing import StandardScaler`.\n",
      "  - Codificar variables categ√≥ricas: `pd.get_dummies(df)`.\n",
      "\n",
      "- **Feature Engineering**:\n",
      "  - Crear nuevas columnas (ej: calcular edad a partir de fecha de nacimiento).\n",
      "  - Reducir dimensionalidad (PCA) si hay muchas columnas.\n",
      "\n",
      "- **Optimizaci√≥n**:\n",
      "  - Convertir tipos de datos para ahorrar memoria: `df[\"columna\"] = df[\"columna\"].astype(\"category\")`.\n",
      "  - Particionar el archivo Parquet si es muy grande.\n",
      "\n",
      "---\n",
      "\n",
      "### Ejemplo pr√°ctico:\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Cargar datos\n",
      "df = pd.read_parquet(\"ventas.parquet\")\n",
      "\n",
      "# 1. Inspecci√≥n inicial\n",
      "print(df.head())\n",
      "print(\"Valores nulos por columna:\", df.isnull().sum())\n",
      "\n",
      "# 2. Limpieza\n",
      "df_clean = df.dropna(subset=[\"precio\"])  # Eliminar filas sin precio\n",
      "\n",
      "# 3. Transformaci√≥n\n",
      "df_clean[\"fecha\"] = pd.to_datetime(df_clean[\"fecha\"])  # Corregir formato de fecha\n",
      "\n",
      "# 4. An√°lisis\n",
      "ventas_por_categoria = df_clean.groupby(\"categor√≠a\")[\"ventas\"].sum()\n",
      "print(ventas_por_categoria)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Herramientas avanzadas:\n",
      "- **Great Expectations**: Para validar la calidad de los datos.\n",
      "- **Polars**: Alternativa m√°s r√°pida a Pandas para grandes conjuntos de datos.\n",
      "- **QuickDA**: Librer√≠a para an√°lisis exploratorio automatizado.\n",
      "\n",
      "Si necesitas ayuda con un caso espec√≠fico o m√°s detalles, ¬°av√≠same! üöÄ\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Como puedo ver los datos de un .parquet o decidir que datos manejar y mejorar \n",
    "\"\"\"\n",
    "\n",
    "response = query_deepseek(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b0b84",
   "metadata": {},
   "source": [
    "Hacer feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c69be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder, \n",
    "    OrdinalEncoder, \n",
    "    StandardScaler, \n",
    "    MinMaxScaler,\n",
    "    KBinsDiscretizer\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing pipeline implementing all feature engineering recommendations\n",
    "    \"\"\"\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "\n",
    "    ## 0. Temporal Sorting (antes de eliminar columnas como 'Season')\n",
    "    season_backup = None\n",
    "    if 'id' in df_processed.columns and 'Season' in df_processed.columns:\n",
    "        season_backup = df_processed['Season'].copy()\n",
    "        df_processed = df_processed.sort_values(['id', 'Season'])\n",
    "\n",
    "    ## 1. Categorical Variable Encoding\n",
    "    binary_cols = ['Sex', 'FGC_CU_Zone']  # Add other binary columns\n",
    "    for col in binary_cols:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed[col] = df_processed[col].astype(int)\n",
    "\n",
    "    # One-hot encoding for nominal categories\n",
    "    nominal_cols = ['Season']  # Add other nominal columns\n",
    "    ohe = OneHotEncoder(drop='first', sparse_output=False)\n",
    "    for col in nominal_cols:\n",
    "        if col in df_processed.columns:\n",
    "            encoded = ohe.fit_transform(df_processed[[col]])\n",
    "            encoded_df = pd.DataFrame(encoded, columns=ohe.get_feature_names_out([col]), index=df_processed.index)\n",
    "            df_processed = pd.concat([df_processed.drop(col, axis=1), encoded_df], axis=1)\n",
    "\n",
    "    # Ordinal encoding for ordered categories\n",
    "    ordinal_mappings = {\n",
    "        'BIA_Activity_Level_num': [1, 2, 3, 4, 5],  # 1=Very Light to 5=Exceptional\n",
    "        'computerinternet_hoursday': [0, 1, 2, 3]    # 0=Less than 1h/day to 3=More than 3hs/day\n",
    "    }\n",
    "    ordinal_encoder = OrdinalEncoder(categories=[ordinal_mappings[col] for col in ordinal_mappings.keys()])\n",
    "    for col in ordinal_mappings:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed[col] = ordinal_encoder.fit_transform(df_processed[[col]])\n",
    "\n",
    "    ## 2. Feature Transformation\n",
    "    if season_backup is not None:\n",
    "        seasons = {'Spring': 0, 'Summer': 1, 'Fall': 2, 'Winter': 3}\n",
    "        df_processed['Season_sin'] = season_backup.map(seasons).apply(lambda x: np.sin(x * (2*np.pi/4)))\n",
    "        df_processed['Season_cos'] = season_backup.map(seasons).apply(lambda x: np.cos(x * (2*np.pi/4)))\n",
    "\n",
    "    skewed_cols = ['BIA_BMR', 'BIA_DEE', 'PAQ_A_Total']\n",
    "    for col in skewed_cols:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed[f'log_{col}'] = np.log1p(df_processed[col])\n",
    "\n",
    "    if 'Age' in df_processed.columns:\n",
    "        binner = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "        df_processed['Age_bin'] = binner.fit_transform(df_processed[['Age']])\n",
    "\n",
    "    ## 3. Composite Features\n",
    "    if all(col in df_processed.columns for col in ['Fitness_Endurance-Time_Mins', 'Time_Sec']):\n",
    "        df_processed['Total_Time_Seconds'] = df_processed['Fitness_Endurance-Time_Mins'] * 60 + df_processed['Time_Sec']\n",
    "\n",
    "    if all(col in df_processed.columns for col in ['FGC_GSD', 'FGC_GSND']):\n",
    "        df_processed['Grip_Strength_Asymmetry'] = (df_processed['FGC_GSD'] - df_processed['FGC_GSND']) / \\\n",
    "                                                  (df_processed['FGC_GSD'] + df_processed['FGC_GSND'] + 1e-6)\n",
    "\n",
    "    if all(col in df_processed.columns for col in ['BMI', 'BIA_Fat']):\n",
    "        df_processed['BMI_Fat_Interaction'] = df_processed['BMI'] * df_processed['BIA_Fat']\n",
    "\n",
    "    ## 4. Domain-Specific Aggregations\n",
    "    fitness_cols = ['FGC_CU', 'FGC_PU', 'FGC_TL']\n",
    "    if all(col in df_processed.columns for col in fitness_cols):\n",
    "        df_processed['Fitness_Composite'] = df_processed[fitness_cols].mean(axis=1)\n",
    "\n",
    "    if all(col in df_processed.columns for col in ['BIA_Fat_Mass', 'Height']):\n",
    "        df_processed['FMI'] = df_processed['BIA_Fat_Mass'] / (df_processed['Height'] / 100)**2\n",
    "\n",
    "    ## 5. Handling Missing Data\n",
    "    cols_with_missing = df_processed.columns[df_processed.isnull().any()].tolist()\n",
    "    for col in cols_with_missing:\n",
    "        df_processed[f'Missing_{col}'] = df_processed[col].isnull().astype(int)\n",
    "\n",
    "    numeric_cols = df_processed.select_dtypes(include=['number']).columns\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df_processed[numeric_cols] = imputer.fit_transform(df_processed[numeric_cols])\n",
    "\n",
    "    ## 6. Normalization/Scaling\n",
    "    to_standard_scale = ['Height', 'Weight']\n",
    "    scaler = StandardScaler()\n",
    "    for col in to_standard_scale:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed[f'{col}_scaled'] = scaler.fit_transform(df_processed[[col]])\n",
    "\n",
    "    to_minmax_scale = ['CGAS_Score']\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    for col in to_minmax_scale:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed[f'{col}_scaled'] = minmax_scaler.fit_transform(df_processed[[col]])\n",
    "\n",
    "    ## 7. Target-Specific Engineering\n",
    "    if all(col in df_processed.columns for col in ['Systolic_BP', 'Diastolic_BP', 'HeartRate']):\n",
    "        df_processed['Cardio_Risk_Score'] = (\n",
    "            df_processed['Systolic_BP'] / 140 + \n",
    "            df_processed['Diastolic_BP'] / 90 + \n",
    "            df_processed['HeartRate'] / 100\n",
    "        )\n",
    "\n",
    "    ## 8. Dimensionality Reduction\n",
    "    correlated_groups = [\n",
    "        ['BIA_TBW', 'BIA_ECW', 'BIA_ICW'],\n",
    "        ['SDS_Total_Raw', 'SDS_Total_T']\n",
    "    ]\n",
    "    for group in correlated_groups:\n",
    "        if all(col in df_processed.columns for col in group):\n",
    "            pca = PCA(n_components=1)\n",
    "            pca_feature = pca.fit_transform(df_processed[group])\n",
    "            group_name = '_'.join(group)\n",
    "            df_processed[f'{group_name}_PCA'] = pca_feature\n",
    "\n",
    "    ## 9. Temporal Feature Derivatives\n",
    "    if 'id' in df_processed.columns:\n",
    "        change_cols = ['BMI', 'Fitness_Composite']\n",
    "        for col in change_cols:\n",
    "            if col in df_processed.columns:\n",
    "                df_processed[f'Delta_{col}'] = df_processed.groupby('id')[col].diff()\n",
    "\n",
    "    ## 10. Textual Data\n",
    "    if 'Description' in df_processed.columns:\n",
    "        df_processed['Dominant_Mentioned'] = df_processed['Description'].str.contains('dominant', case=False).astype(int)\n",
    "\n",
    "    return df_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cdc77a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "' FeatureProcessor class for preprocessing data with text and categorical features'\n",
    "class FeatureProcessor:\n",
    "    def __init__(self):\n",
    "        self.encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        self.text_cols = []\n",
    "        self.cat_cols = []\n",
    "        self.model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "        self.embed_cols = []\n",
    "        \n",
    "    def preprocess(self, train, test):\n",
    "        'Optimized preprocessing to avoid fragmentation'\n",
    "        try:\n",
    "            self._identify_safe_columns(train, test)\n",
    "            \n",
    "            if self.text_cols:\n",
    "                train, test = self._process_text_optimized(train, test)\n",
    "            \n",
    "            if self.cat_cols:\n",
    "                train, test = self._encode_categoricals(train, test)\n",
    "            # normalization\n",
    "            train, test = self._scale_numerical(train, test)\n",
    "            \n",
    "            return train, test\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error on preprocesing: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _identify_safe_columns(self, train, test):\n",
    "        \"\"\"Identifica columnas existentes en ambos datasets\"\"\"\n",
    "        common_cols = list(set(train.columns) & set(test.columns))\n",
    "        \n",
    "        self.text_cols = [\n",
    "            col for col in common_cols \n",
    "            if train[col].dtype == 'object' \n",
    "            and train[col].str.contains('[a-zA-Z]', regex=True, na=False).any()\n",
    "        ]\n",
    "        \n",
    "        self.cat_cols = [\n",
    "            col for col in common_cols \n",
    "            if train[col].dtype == 'object' \n",
    "            and col not in self.text_cols\n",
    "        ]\n",
    "    \n",
    "    def _process_text_optimized(self, train, test):\n",
    "        'procesing of text without fragmentation'\n",
    "        train_embeddings = []\n",
    "        test_embeddings = []\n",
    "        \n",
    "        for col in self.text_cols:\n",
    "            train_text = train[col].fillna('').astype(str)\n",
    "            test_text = test[col].fillna('').astype(str)\n",
    "            \n",
    "            # Embeddings for train and test\n",
    "            train_emb = self.model.encode(train_text.tolist(), show_progress_bar=False)\n",
    "            test_emb = self.model.encode(test_text.tolist(), show_progress_bar=False)\n",
    "            \n",
    "            train_embeddings.append(train_emb)\n",
    "            test_embeddings.append(test_emb)\n",
    "        \n",
    "        # concatenate all embeddings horizontally\n",
    "        if train_embeddings:\n",
    "            train_embeddings = np.hstack(train_embeddings)\n",
    "            test_embeddings = np.hstack(test_embeddings)\n",
    "            \n",
    "            # Create dataframes before assigning\n",
    "            n_features = train_embeddings.shape[1]\n",
    "            self.embed_cols = [f'text_embed_{i}' for i in range(n_features)]\n",
    "            \n",
    "            train_emb_df = pd.DataFrame(train_embeddings, columns=self.embed_cols, index=train.index)\n",
    "            test_emb_df = pd.DataFrame(test_embeddings, columns=self.embed_cols, index=test.index)\n",
    "            \n",
    "            # Concatenate embeddings\n",
    "            train = pd.concat([train, train_emb_df], axis=1)\n",
    "            test = pd.concat([test, test_emb_df], axis=1)\n",
    "            \n",
    "        return train, test\n",
    "    \n",
    "    def _encode_categoricals(self, train, test):\n",
    "        \"\"\"codification of categoricals\"\"\"\n",
    "        if self.cat_cols:\n",
    "            train_cats = train[self.cat_cols]\n",
    "            test_cats = test[self.cat_cols]\n",
    "            \n",
    "            # Ensure both datasets have the same categories\n",
    "            train[self.cat_cols] = self.encoder.fit_transform(train_cats)\n",
    "            test[self.cat_cols] = self.encoder.transform(test_cats)\n",
    "            \n",
    "        return train, test\n",
    "    \n",
    "    def _scale_numerical(self, train, test):\n",
    "        num_cols = [col for col in train.select_dtypes(include=np.number).columns \n",
    "                   if col not in ['Subject_ID', 'PCIAT-PCIAT_Total'] and col in test.columns]\n",
    "        \n",
    "        if num_cols:\n",
    "            means = train[num_cols].mean()\n",
    "            stds = train[num_cols].std() + 1e-8\n",
    "            \n",
    "            train[num_cols] = (train[num_cols] - means) / stds\n",
    "            test[num_cols] = (test[num_cols] - means) / stds\n",
    "            \n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23643ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "' Summarize actigraphy data with enhanced statistical features'\n",
    "def summarize_actigraphy(df, subject_id_col='Subject_ID'):\n",
    "    \"\"\"\n",
    "    Enhanced actigraphy processing with:\n",
    "    - Percentiles (10th, 25th, 75th, 90th)\n",
    "    - Robust statistical measures (IQR, MAD)\n",
    "    - Frequency domain features (FFT)\n",
    "    \"\"\"\n",
    "    exclude_cols = [subject_id_col, 'timestamp']\n",
    "    num_cols = [col for col in df.columns \n",
    "               if col not in exclude_cols \n",
    "               and pd.api.types.is_numeric_dtype(df[col])]\n",
    "    \n",
    "    # Time-domain features\n",
    "    stats = {\n",
    "        'mean': np.mean,\n",
    "        'std': np.std,\n",
    "        'min': np.min,\n",
    "        'max': np.max,\n",
    "        'median': np.median,\n",
    "        'skew': skew,\n",
    "        'kurtosis': kurtosis,\n",
    "        'q1': lambda x: np.percentile(x, 25),\n",
    "        'q3': lambda x: np.percentile(x, 75),\n",
    "        'iqr': lambda x: np.percentile(x, 75) - np.percentile(x, 25),\n",
    "        'mad': lambda x: np.median(np.abs(x - np.median(x)))\n",
    "    }\n",
    "    \n",
    "    # Frequency-domain features (simplified FFT)\n",
    "    def dominant_freq(x):\n",
    "        if len(x) < 2: return 0\n",
    "        fft = np.abs(np.fft.fft(x))\n",
    "        return np.argmax(fft[1:len(fft)//2]) + 1\n",
    "    \n",
    "    summary = df.groupby(subject_id_col)[num_cols].agg(stats)\n",
    "    summary.columns = [f'{col}_{stat}' for col, stat in summary.columns]\n",
    "    \n",
    "    # Add frequency features\n",
    "    freq_features = df.groupby(subject_id_col)[num_cols].agg(dominant_freq)\n",
    "    freq_features.columns = [f'{col}_dominant_freq' for col in freq_features.columns]\n",
    "    \n",
    "    return pd.concat([summary, freq_features], axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f233bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_features(df, subject_col='Subject_ID'):\n",
    "    \"\"\"Extracts 15 key time features per subject\"\"\"\n",
    "    features = []\n",
    "    for subject_id, group in df.groupby(subject_col):\n",
    "        if 'timestamp' in group.columns:\n",
    "            time_diff = group['timestamp'].diff().dt.total_seconds()\n",
    "            feat = {\n",
    "                'Subject_ID': subject_id,\n",
    "                'total_events': len(group),\n",
    "                'active_hours': (time_diff < 3600).sum(),\n",
    "                'night_activity': group[group['timestamp'].dt.hour.between(0, 6)]['value'].mean(),\n",
    "                'max_activity': group['value'].max(),\n",
    "                'std_activity': group['value'].std(),\n",
    "            }\n",
    "            features.append(feat)\n",
    "    return pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f6ed58",
   "metadata": {},
   "source": [
    "Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01c5d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "' Train and evaluate the model with optimized parameters'\n",
    "def train_and_evaluate(X, y):\n",
    "    model = XGBClassifier(\n",
    "        objective='multi:softmax',\n",
    "        num_class=len(np.unique(y)),\n",
    "        n_estimators=150,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=0.5,\n",
    "        tree_method='hist',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    qwk_scores = []\n",
    "    \"\"\"    Stratified K-Fold cross-validation to ensure balanced class distribution\"\"\"\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        # Fit the model\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=0\n",
    "        )\n",
    "        preds = model.predict(X_val)\n",
    "        qwk_scores.append(cohen_kappa_score(y_val, preds, weights='quadratic'))\n",
    "    \n",
    "    return model, np.mean(qwk_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7265a910",
   "metadata": {},
   "source": [
    "Complete the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58a2ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Utility functions for competition submission\n",
    "'''\n",
    "\n",
    "def save_submission(test, preds, output_dir):\n",
    "    '''\n",
    "    Saves predictions in Kaggle submission format\n",
    "    \n",
    "    Args:\n",
    "        test: Test DataFrame\n",
    "        preds: Model predictions\n",
    "        output_dir: Directory to save submission file\n",
    "    '''\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test['id'],\n",
    "        'sii': preds\n",
    "    })\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    submission_path = os.path.join(output_dir, 'submission.csv')\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1c21e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727e2827",
   "metadata": {},
   "outputs": [],
   "source": [
    "' Main function to orchestrate the workflow'\n",
    "def main():\n",
    "    try:\n",
    "        print(\"\\n=== Loading data ===\")\n",
    "        train = pd.read_csv(train_path)\n",
    "        test = pd.read_csv(test_path)\n",
    "        \n",
    "        # Verification of critical data\n",
    "        assert 'PCIAT-PCIAT_Total' in train.columns, \"Target column not found in train data\"\n",
    "        train = train.dropna(subset=['PCIAT-PCIAT_Total'])\n",
    "        \n",
    "        print(\"\\n=== Preprocessing ===\")\n",
    "        processor = FeatureProcessor()\n",
    "        train, test = processor.preprocess(train, test)\n",
    "    \n",
    "        \n",
    "        print(\"\\n=== Preparing target ===\")\n",
    "        # Robust version of qcut\n",
    "        train['SII_group'], bins = pd.qcut(\n",
    "            train['PCIAT-PCIAT_Total'],\n",
    "            q=4,\n",
    "            labels=[0, 1, 2, 3],\n",
    "            retbins=True,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        y = train['SII_group'].astype(int)\n",
    "        \n",
    "        print(\"\\n=== Selecting features ===\")\n",
    "        # Exclude irrelevant columns and ensure consistency\n",
    "        exclude = ['PCIAT-PCIAT_Total', 'Subject_ID', 'SII_group', 'timestamp', 'id']\n",
    "        \n",
    "        # Only features present in both datasets\n",
    "        common_features = list(set(train.columns) & set(test.columns))\n",
    "        features = [\n",
    "            col for col in common_features\n",
    "            if col not in exclude\n",
    "            and pd.api.types.is_numeric_dtype(train[col])\n",
    "            and col in test.columns\n",
    "        ]\n",
    "        \n",
    "        print(f\"Selected features: {len(features)}\")\n",
    "        X = train[features]\n",
    "        \n",
    "        print(\"\\n=== Training ===\")\n",
    "        model, qwk = train_and_evaluate(X, y)\n",
    "        print(f\"\\n‚úî QWK average: {qwk:.4f}\")\n",
    "        \n",
    "        print(\"\\n=== Generating submission ===\")\n",
    "        # Check features in test\n",
    "        missing_in_test = [col for col in features if col not in test.columns]\n",
    "        if missing_in_test:\n",
    "            print(f\"‚ö† Features faltantes en test: {missing_in_test}\")\n",
    "            features = [col for col in features if col in test.columns]\n",
    "        \n",
    "        test_preds = model.predict(test[features])\n",
    "        \n",
    "        # Ensure 'id' column exists in test DataFrame\n",
    "        if 'id' not in test.columns and 'Subject_ID' in test.columns:\n",
    "            test['id'] = test['Subject_ID']\n",
    "        elif 'id' not in test.columns:\n",
    "            test['id'] = range(len(test))\n",
    "        \n",
    "        save_submission(test, test_preds, OUTPUT_DIR)\n",
    "        print(f\"Submission generated in {OUTPUT_DIR / 'submission.csv'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nCritical error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0919f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        print(\"\\n=== Loading data ===\")\n",
    "        train = pd.read_csv(train_path)\n",
    "        test = pd.read_csv(test_path)\n",
    "        \n",
    "        assert 'PCIAT-PCIAT_Total' in train.columns, \"Target column not found in train data\"\n",
    "        train = train.dropna(subset=['PCIAT-PCIAT_Total'])\n",
    "\n",
    "        print(\"\\n=== General Feature Engineering ===\")\n",
    "        train = preprocess_data(train)\n",
    "        test = preprocess_data(test)\n",
    "\n",
    "        print(\"\\n=== Advanced Preprocessing (Text + Categoricals) ===\")\n",
    "        processor = FeatureProcessor()\n",
    "        train, test = processor.preprocess(train, test)\n",
    "\n",
    "        print(\"\\n=== Preparing target ===\")\n",
    "        train['SII_group'], bins = pd.qcut(\n",
    "            train['PCIAT-PCIAT_Total'],\n",
    "            q=4,\n",
    "            labels=[0, 1, 2, 3],\n",
    "            retbins=True,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        y = train['SII_group'].astype(int)\n",
    "\n",
    "        print(\"\\n=== Selecting features ===\")\n",
    "        exclude = ['PCIAT-PCIAT_Total', 'Subject_ID', 'SII_group', 'timestamp', 'id']\n",
    "        common_features = list(set(train.columns) & set(test.columns))\n",
    "        features = [\n",
    "            col for col in common_features\n",
    "            if col not in exclude and pd.api.types.is_numeric_dtype(train[col])\n",
    "        ]\n",
    "\n",
    "        print(f\"Selected features: {len(features)}\")\n",
    "        X = train[features]\n",
    "\n",
    "        print(\"\\n=== Training ===\")\n",
    "        model, qwk = train_and_evaluate(X, y)\n",
    "        print(f\"\\n‚úî QWK average: {qwk:.4f}\")\n",
    "\n",
    "        print(\"\\n=== Generating submission ===\")\n",
    "        if 'id' not in test.columns and 'Subject_ID' in test.columns:\n",
    "            test['id'] = test['Subject_ID']\n",
    "        elif 'id' not in test.columns:\n",
    "            test['id'] = range(len(test))\n",
    "\n",
    "        test_preds = model.predict(test[features])\n",
    "        save_submission(test, test_preds, OUTPUT_DIR)\n",
    "        print(f\"Submission generated in {OUTPUT_DIR / 'submission.csv'}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCritical error: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "caabfbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading data ===\n",
      "\n",
      "=== General Feature Engineering ===\n",
      "\n",
      "=== Advanced Preprocessing (Text + Categoricals) ===\n",
      "\n",
      "=== Preparing target ===\n",
      "\n",
      "=== Selecting features ===\n",
      "Selected features: 4327\n",
      "\n",
      "=== Training ===\n",
      "\n",
      "‚úî QWK average: 0.4255\n",
      "\n",
      "=== Generating submission ===\n",
      "Submission saved to C:\\TrabajoFinal\\Child_mind_institute_problematic_internet_use\\src\\outputs\\submission.csv\n",
      "Submission generated in C:\\TrabajoFinal\\Child_mind_institute_problematic_internet_use\\src\\outputs\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
