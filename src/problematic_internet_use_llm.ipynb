{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b738c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de19fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths robustos\n",
    "BASE_DIR = Path().resolve()\n",
    "SRC_DIR = BASE_DIR / \"src\"\n",
    "DATA_DIR = SRC_DIR / \"data\"\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "\n",
    "# Agregar src/ al path para importar módulos locales\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a961ffb",
   "metadata": {},
   "source": [
    "Importacion de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5594db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir='data'):\n",
    "    \"\"\"Carga datos desde la carpeta 'data' con paths relativos seguros\"\"\"\n",
    "    try:\n",
    "        # Verifica existencia de archivos\n",
    "        train_csv_path = os.path.join(data_dir, 'train.csv')\n",
    "        test_csv_path = os.path.join(data_dir, 'test.csv')\n",
    "        train_parquet_path = os.path.join(data_dir, 'train.parquet')\n",
    "        test_parquet_path = os.path.join(data_dir, 'test.parquet')\n",
    "        \n",
    "        if not all(os.path.exists(f) for f in [train_csv_path, test_csv_path]):\n",
    "            raise FileNotFoundError(\"Archivos CSV no encontrados en la carpeta 'data'\")\n",
    "        \n",
    "        # Carga CSV\n",
    "        train_csv = pd.read_csv(train_csv_path)\n",
    "        test_csv = pd.read_csv(test_csv_path)\n",
    "        \n",
    "        # Carga Parquet si existen\n",
    "        train_parquet = pd.DataFrame()\n",
    "        test_parquet = pd.DataFrame()\n",
    "        if os.path.exists(train_parquet_path):\n",
    "            train_parquet = pq.read_table(train_parquet_path).to_pandas()\n",
    "        if os.path.exists(test_parquet_path):\n",
    "            test_parquet = pq.read_table(test_parquet_path).to_pandas()\n",
    "        \n",
    "        # Combina datos\n",
    "        train = pd.merge(train_csv, train_parquet, on='Subject_ID', how='left') if not train_parquet.empty else train_csv\n",
    "        test = pd.merge(test_csv, test_parquet, on='Subject_ID', how='left') if not test_parquet.empty else test_csv\n",
    "        \n",
    "        return train, test, {}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error cargando datos: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf01c1",
   "metadata": {},
   "source": [
    "LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57b5ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Carga las variables desde .env\n",
    "\n",
    "api_key = os.getenv(\"DEEPSEEK_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4acffd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"DEEPSEEK_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efc004d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_deepseek(prompt: str, model: str = \"deepseek/deepseek-chat-v3-0324:free\") -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\":\"user\", \"content\":prompt}],\n",
    "        temperature=0.6,\n",
    "        # Muestra la cantidad de output\n",
    "        max_tokens=4096,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76e22533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celdas encontradas: 1\n"
     ]
    }
   ],
   "source": [
    "# Cargar el notebook\n",
    "from langchain_community.document_loaders import NotebookLoader\n",
    "\n",
    "loader = NotebookLoader(\"problematic_internet_use_llm.ipynb\", include_outputs=False, remove_newline=True)\n",
    "docs = loader.load()\n",
    "print(f\"Celdas encontradas: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a03c557",
   "metadata": {},
   "source": [
    "Prompts que hay que realizar de manera general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d5864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Código mejorado con cambios de estilo y eficiencia\n",
      "\n",
      "```python\n",
      "# Importación del cargador de notebooks de LangChain\n",
      "from langchain_community.document_loaders import NotebookLoader\n",
      "\n",
      "# Configuración del cargador:\n",
      "# - Archivo: prueba-llm.ipynb\n",
      "# - exclude_outputs: no incluir salidas de celdas\n",
      "# - remove_newline: elimina saltos de línea innecesarios\n",
      "loader = NotebookLoader(\n",
      "    \"prueba-llm.ipynb\", \n",
      "    include_outputs=False, \n",
      "    remove_newline=True\n",
      ")\n",
      "\n",
      "# Carga los documentos (celdas del notebook)\n",
      "docs = loader.load()\n",
      "\n",
      "# Muestra el número de celdas encontradas\n",
      "print(f\"Celdas encontradas: {len(docs)}\")\n",
      "```\n",
      "\n",
      "# Cambios realizados:\n",
      "# 1. Organización más clara del código con saltos de línea lógicos\n",
      "# 2. Comentarios descriptivos para cada sección\n",
      "# 3. Formato consistente en la asignación de parámetros\n",
      "# 4. Se mantuvo exactamente la misma funcionalidad original\n",
      "# 5. Se agregaron comentarios explicando el propósito de cada parámetro\n"
     ]
    }
   ],
   "source": [
    "codigo_original = \"\"\"\n",
    "# Carga el notebook activo (puedes ver su nombre en Kaggle)\n",
    "from langchain_community.document_loaders import NotebookLoader\n",
    "\n",
    "loader = NotebookLoader(\"prueba-llm.ipynb\", include_outputs=False, remove_newline=True)\n",
    "docs = loader.load()\n",
    "print(f\"Celdas encontradas: {len(docs)}\")\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Reescribe el siguiente código con mejoras de estilo y eficiencia, sin cambiar su funcionalidad:\\n{codigo_original},\n",
    "ademas, todo lo que no este relacionado a codigo lo dejes comentado, incluyendo los cambios realizados\"\"\"\n",
    "\n",
    "respuesta = query_deepseek(prompt)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b0b84",
   "metadata": {},
   "source": [
    "Hacer feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdc77a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureProcessor:\n",
    "    def __init__(self):\n",
    "        self.encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        self.text_cols = []\n",
    "        self.cat_cols = []\n",
    "        self.model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "        self.embed_cols = []\n",
    "        \n",
    "    def preprocess(self, train, test):\n",
    "        \"\"\"Versión optimizada que evita fragmentación\"\"\"\n",
    "        try:\n",
    "            # 1. Identificar columnas seguras\n",
    "            self._identify_safe_columns(train, test)\n",
    "            \n",
    "            # 2. Procesar texto\n",
    "            if self.text_cols:\n",
    "                train, test = self._process_text_optimized(train, test)\n",
    "            \n",
    "            # 3. Codificar categóricas\n",
    "            if self.cat_cols:\n",
    "                train, test = self._encode_categoricals(train, test)\n",
    "            \n",
    "            # 4. Normalizar numéricas\n",
    "            train, test = self._scale_numerical(train, test)\n",
    "            \n",
    "            return train, test\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en preprocesamiento: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _identify_safe_columns(self, train, test):\n",
    "        \"\"\"Identifica columnas existentes en ambos datasets\"\"\"\n",
    "        common_cols = list(set(train.columns) & set(test.columns))\n",
    "        \n",
    "        self.text_cols = [\n",
    "            col for col in common_cols \n",
    "            if train[col].dtype == 'object' \n",
    "            and train[col].str.contains('[a-zA-Z]', regex=True, na=False).any()\n",
    "        ]\n",
    "        \n",
    "        self.cat_cols = [\n",
    "            col for col in common_cols \n",
    "            if train[col].dtype == 'object' \n",
    "            and col not in self.text_cols\n",
    "        ]\n",
    "    \n",
    "    def _process_text_optimized(self, train, test):\n",
    "        \"\"\"Procesamiento de texto sin fragmentación\"\"\"\n",
    "        # Generar todos los embeddings primero\n",
    "        train_embeddings = []\n",
    "        test_embeddings = []\n",
    "        \n",
    "        for col in self.text_cols:\n",
    "            train_text = train[col].fillna('').astype(str)\n",
    "            test_text = test[col].fillna('').astype(str)\n",
    "            \n",
    "            # Embeddings para train y test\n",
    "            train_emb = self.model.encode(train_text.tolist(), show_progress_bar=False)\n",
    "            test_emb = self.model.encode(test_text.tolist(), show_progress_bar=False)\n",
    "            \n",
    "            train_embeddings.append(train_emb)\n",
    "            test_embeddings.append(test_emb)\n",
    "        \n",
    "        # Concatenar todos los embeddings horizontalmente\n",
    "        if train_embeddings:\n",
    "            train_embeddings = np.hstack(train_embeddings)\n",
    "            test_embeddings = np.hstack(test_embeddings)\n",
    "            \n",
    "            # Crear DataFrames completos antes de asignar\n",
    "            n_features = train_embeddings.shape[1]\n",
    "            self.embed_cols = [f'text_embed_{i}' for i in range(n_features)]\n",
    "            \n",
    "            train_emb_df = pd.DataFrame(train_embeddings, columns=self.embed_cols, index=train.index)\n",
    "            test_emb_df = pd.DataFrame(test_embeddings, columns=self.embed_cols, index=test.index)\n",
    "            \n",
    "            # Concatenar de una sola vez\n",
    "            train = pd.concat([train, train_emb_df], axis=1)\n",
    "            test = pd.concat([test, test_emb_df], axis=1)\n",
    "            \n",
    "        return train, test\n",
    "    \n",
    "    def _encode_categoricals(self, train, test):\n",
    "        \"\"\"Codificación segura de categóricas\"\"\"\n",
    "        if self.cat_cols:\n",
    "            train_cats = train[self.cat_cols]\n",
    "            test_cats = test[self.cat_cols]\n",
    "            \n",
    "            # Codificar y asignar de una vez\n",
    "            train[self.cat_cols] = self.encoder.fit_transform(train_cats)\n",
    "            test[self.cat_cols] = self.encoder.transform(test_cats)\n",
    "            \n",
    "        return train, test\n",
    "    \n",
    "    def _scale_numerical(self, train, test):\n",
    "        \"\"\"Normalización optimizada\"\"\"\n",
    "        num_cols = [col for col in train.select_dtypes(include=np.number).columns \n",
    "                   if col not in ['Subject_ID', 'PCIAT-PCIAT_Total'] and col in test.columns]\n",
    "        \n",
    "        if num_cols:\n",
    "            means = train[num_cols].mean()\n",
    "            stds = train[num_cols].std() + 1e-8\n",
    "            \n",
    "            train[num_cols] = (train[num_cols] - means) / stds\n",
    "            test[num_cols] = (test[num_cols] - means) / stds\n",
    "            \n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f23643ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_actigraphy(df, subject_id_col='Subject_ID'):\n",
    "    \"\"\"\n",
    "    Enhanced actigraphy processing with:\n",
    "    - Percentiles (10th, 25th, 75th, 90th)\n",
    "    - Robust statistical measures (IQR, MAD)\n",
    "    - Frequency domain features (FFT)\n",
    "    \"\"\"\n",
    "    exclude_cols = [subject_id_col, 'timestamp']\n",
    "    num_cols = [col for col in df.columns \n",
    "               if col not in exclude_cols \n",
    "               and pd.api.types.is_numeric_dtype(df[col])]\n",
    "    \n",
    "    # Time-domain features\n",
    "    stats = {\n",
    "        'mean': np.mean,\n",
    "        'std': np.std,\n",
    "        'min': np.min,\n",
    "        'max': np.max,\n",
    "        'median': np.median,\n",
    "        'skew': skew,\n",
    "        'kurtosis': kurtosis,\n",
    "        'q1': lambda x: np.percentile(x, 25),\n",
    "        'q3': lambda x: np.percentile(x, 75),\n",
    "        'iqr': lambda x: np.percentile(x, 75) - np.percentile(x, 25),\n",
    "        'mad': lambda x: np.median(np.abs(x - np.median(x)))\n",
    "    }\n",
    "    \n",
    "    # Frequency-domain features (simplified FFT)\n",
    "    def dominant_freq(x):\n",
    "        if len(x) < 2: return 0\n",
    "        fft = np.abs(np.fft.fft(x))\n",
    "        return np.argmax(fft[1:len(fft)//2]) + 1\n",
    "    \n",
    "    summary = df.groupby(subject_id_col)[num_cols].agg(stats)\n",
    "    summary.columns = [f'{col}_{stat}' for col, stat in summary.columns]\n",
    "    \n",
    "    # Add frequency features\n",
    "    freq_features = df.groupby(subject_id_col)[num_cols].agg(dominant_freq)\n",
    "    freq_features.columns = [f'{col}_dominant_freq' for col in freq_features.columns]\n",
    "    \n",
    "    return pd.concat([summary, freq_features], axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f233bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_features(df, subject_col='Subject_ID'):\n",
    "    \"\"\"Extrae 15 features temporales clave por sujeto\"\"\"\n",
    "    features = []\n",
    "    for subject_id, group in df.groupby(subject_col):\n",
    "        if 'timestamp' in group.columns:\n",
    "            time_diff = group['timestamp'].diff().dt.total_seconds()\n",
    "            feat = {\n",
    "                'Subject_ID': subject_id,\n",
    "                'total_events': len(group),\n",
    "                'active_hours': (time_diff < 3600).sum(),\n",
    "                'night_activity': group[group['timestamp'].dt.hour.between(0, 6)]['value'].mean(),\n",
    "                'max_activity': group['value'].max(),\n",
    "                'std_activity': group['value'].std(),\n",
    "            }\n",
    "            features.append(feat)\n",
    "    return pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a40a74",
   "metadata": {},
   "source": [
    "LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e536e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19f6ed58",
   "metadata": {},
   "source": [
    "Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01c5d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X, y):\n",
    "    \"\"\"Versión optimizada sin warnings\"\"\"\n",
    "    model = XGBClassifier(\n",
    "        objective='multi:softmax',\n",
    "        num_class=len(np.unique(y)),\n",
    "        n_estimators=150,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=0.5,\n",
    "        tree_method='hist',  # Más eficiente\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    qwk_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=0\n",
    "        )\n",
    "        preds = model.predict(X_val)\n",
    "        qwk_scores.append(cohen_kappa_score(y_val, preds, weights='quadratic'))\n",
    "    \n",
    "    return model, np.mean(qwk_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e23c75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Complete the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58a2ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Utility functions for competition submission\n",
    "'''\n",
    "\n",
    "def save_submission(test, preds, output_dir):\n",
    "    '''\n",
    "    Saves predictions in Kaggle submission format\n",
    "    \n",
    "    Args:\n",
    "        test: Test DataFrame\n",
    "        preds: Model predictions\n",
    "        output_dir: Directory to save submission file\n",
    "    '''\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test['id'],\n",
    "        'sii': preds\n",
    "    })\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    submission_path = os.path.join(output_dir, 'submission.csv')\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "727e2827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cargando datos ===\n",
      "\n",
      "=== Preprocesamiento ===\n",
      "\n",
      "=== Preparando target ===\n",
      "\n",
      "=== Seleccionando features ===\n",
      "Features seleccionadas: 4272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leosa\\AppData\\Local\\Temp\\ipykernel_9280\\4003652640.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['SII_group'], bins = pd.qcut(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Entrenamiento ===\n",
      "\n",
      "✔ QWK promedio: 0.4310\n",
      "\n",
      "=== Generando submission ===\n",
      "Submission saved to C:\\TrabajoFinal\\Child_mind_institute_problematic_internet_use\\outputs\\submission.csv\n",
      "✔ Submission generado en C:\\TrabajoFinal\\Child_mind_institute_problematic_internet_use\\outputs\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Directorios de datos y salida\n",
    "DATA_DIR = SRC_DIR / \"data\"\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "def main():\n",
    "    try:\n",
    "        print(\"\\n=== Cargando datos ===\")\n",
    "        train = pd.read_csv(DATA_DIR / 'train.csv')\n",
    "        test = pd.read_csv(DATA_DIR / 'test.csv')\n",
    "        \n",
    "        # Verificación de datos críticos\n",
    "        assert 'PCIAT-PCIAT_Total' in train.columns, \"Columna target no encontrada\"\n",
    "        train = train.dropna(subset=['PCIAT-PCIAT_Total'])\n",
    "        \n",
    "        print(\"\\n=== Preprocesamiento ===\")\n",
    "        processor = FeatureProcessor()\n",
    "        train, test = processor.preprocess(train, test)\n",
    "        \n",
    "        print(\"\\n=== Preparando target ===\")\n",
    "        # Versión robusta de qcut\n",
    "        train['SII_group'], bins = pd.qcut(\n",
    "            train['PCIAT-PCIAT_Total'],\n",
    "            q=4,\n",
    "            labels=[0, 1, 2, 3],\n",
    "            retbins=True,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        y = train['SII_group'].astype(int)\n",
    "        \n",
    "        print(\"\\n=== Seleccionando features ===\")\n",
    "        # Excluir columnas no relevantes y asegurar consistencia\n",
    "        exclude = ['PCIAT-PCIAT_Total', 'Subject_ID', 'SII_group', 'timestamp', 'id']\n",
    "        \n",
    "        # Solo features presentes en ambos datasets\n",
    "        common_features = list(set(train.columns) & set(test.columns))\n",
    "        features = [\n",
    "            col for col in common_features\n",
    "            if col not in exclude\n",
    "            and pd.api.types.is_numeric_dtype(train[col])\n",
    "            and col in test.columns\n",
    "        ]\n",
    "        \n",
    "        print(f\"Features seleccionadas: {len(features)}\")\n",
    "        X = train[features]\n",
    "        \n",
    "        print(\"\\n=== Entrenamiento ===\")\n",
    "        model, qwk = train_and_evaluate(X, y)\n",
    "        print(f\"\\n✔ QWK promedio: {qwk:.4f}\")\n",
    "        \n",
    "        print(\"\\n=== Generando submission ===\")\n",
    "        # Verificar features en test\n",
    "        missing_in_test = [col for col in features if col not in test.columns]\n",
    "        if missing_in_test:\n",
    "            print(f\"⚠ Features faltantes en test: {missing_in_test}\")\n",
    "            features = [col for col in features if col in test.columns]\n",
    "        \n",
    "        test_preds = model.predict(test[features])\n",
    "        \n",
    "        # Asegurar ID para submission\n",
    "        if 'id' not in test.columns and 'Subject_ID' in test.columns:\n",
    "            test['id'] = test['Subject_ID']\n",
    "        elif 'id' not in test.columns:\n",
    "            test['id'] = range(len(test))\n",
    "        \n",
    "        save_submission(test, test_preds, OUTPUT_DIR)\n",
    "        print(f\"✔ Submission generado en {OUTPUT_DIR / 'submission.csv'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error crítico: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
