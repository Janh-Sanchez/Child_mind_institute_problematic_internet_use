{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b738c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "de19fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Definir directorios robustamente desde notebook\n",
    "BASE_DIR = Path().resolve()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "\n",
    "# 2. Agregar src al path para poder importar módulos locales\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "# 3. Verificar lectura de los archivos\n",
    "train_path = DATA_DIR / 'train.csv'\n",
    "test_path = DATA_DIR / 'test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a961ffb",
   "metadata": {},
   "source": [
    "Importacion de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5594db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir='data'):\n",
    "    \"\"\"Carga datos desde la carpeta 'data' con paths relativos seguros\"\"\"\n",
    "    try:\n",
    "        # Verifica existencia de archivos\n",
    "        train_csv_path = os.path.join(data_dir, 'train.csv')\n",
    "        test_csv_path = os.path.join(data_dir, 'test.csv')\n",
    "        train_parquet_path = os.path.join(data_dir, 'train.parquet')\n",
    "        test_parquet_path = os.path.join(data_dir, 'test.parquet')\n",
    "        \n",
    "        if not all(os.path.exists(f) for f in [train_csv_path, test_csv_path]):\n",
    "            raise FileNotFoundError(\"Archivos CSV no encontrados en la carpeta 'data'\")\n",
    "        \n",
    "        # Carga CSV\n",
    "        train_csv = pd.read_csv(train_csv_path)\n",
    "        test_csv = pd.read_csv(test_csv_path)\n",
    "        \n",
    "        # Carga Parquet si existen\n",
    "        train_parquet = pd.DataFrame()\n",
    "        test_parquet = pd.DataFrame()\n",
    "        if os.path.exists(train_parquet_path):\n",
    "            train_parquet = pq.read_table(train_parquet_path).to_pandas()\n",
    "        if os.path.exists(test_parquet_path):\n",
    "            test_parquet = pq.read_table(test_parquet_path).to_pandas()\n",
    "        \n",
    "        # Combina datos\n",
    "        train = pd.merge(train_csv, train_parquet, on='Subject_ID', how='left') if not train_parquet.empty else train_csv\n",
    "        test = pd.merge(test_csv, test_parquet, on='Subject_ID', how='left') if not test_parquet.empty else test_csv\n",
    "        \n",
    "        return train, test, {}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error cargando datos: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf01c1",
   "metadata": {},
   "source": [
    "LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57b5ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Carga las variables desde .env\n",
    "\n",
    "api_key = os.getenv(\"DEEPSEEK_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4acffd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"DEEPSEEK_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "efc004d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_deepseek(prompt: str, model: str = \"deepseek/deepseek-chat-v3-0324:free\") -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\":\"user\", \"content\":prompt}],\n",
    "        temperature=0.6,\n",
    "        # Muestra la cantidad de output\n",
    "        max_tokens=4096,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76e22533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celdas encontradas: 1\n"
     ]
    }
   ],
   "source": [
    "# Cargar el notebook\n",
    "from langchain_community.document_loaders import NotebookLoader\n",
    "\n",
    "loader = NotebookLoader(\"problematic_internet_use_llm.ipynb\", include_outputs=False, remove_newline=True)\n",
    "docs = loader.load()\n",
    "print(f\"Celdas encontradas: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a03c557",
   "metadata": {},
   "source": [
    "Prompts que hay que realizar de manera general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee2d5864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "# Importa el cargador de notebooks de LangChain\n",
      "from langchain_community.document_loaders import NotebookLoader\n",
      "\n",
      "# Configuración del cargador:\n",
      "# - Archivo: prueba-llm.ipynb\n",
      "# - exclude_outputs: No incluir salidas de celdas\n",
      "# - remove_newline: Eliminar saltos de línea\n",
      "loader = NotebookLoader(\n",
      "    file_path=\"prueba-llm.ipynb\",\n",
      "    include_outputs=False,\n",
      "    remove_newline=True\n",
      ")\n",
      "\n",
      "# Carga los documentos y muestra el conteo de celdas\n",
      "docs = loader.load()\n",
      "print(f\"Celdas encontradas: {len(docs)}\")\n",
      "\n",
      "# Mejoras realizadas:\n",
      "# 1. Formateo consistente del código\n",
      "# 2. Argumentos nombrados explícitamente\n",
      "# 3. Comentarios descriptivos\n",
      "# 4. Mantenimiento de la funcionalidad original\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "codigo_original = \"\"\"\n",
    "# Carga el notebook activo (puedes ver su nombre en Kaggle)\n",
    "from langchain_community.document_loaders import NotebookLoader\n",
    "\n",
    "loader = NotebookLoader(\"prueba-llm.ipynb\", include_outputs=False, remove_newline=True)\n",
    "docs = loader.load()\n",
    "print(f\"Celdas encontradas: {len(docs)}\")\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Reescribe el siguiente código con mejoras de estilo y eficiencia, sin cambiar su funcionalidad:\\n{codigo_original},\n",
    "ademas, todo lo que no este relacionado a codigo lo dejes comentado, incluyendo los cambios realizados\"\"\"\n",
    "\n",
    "respuesta = query_deepseek(prompt)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8a36b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_archivo(filepath: str, pregunta: str):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        contenido = f.read()\n",
    "\n",
    "    prompt = f\"\"\"Tengo este archivo llamado `{filepath}` con el siguiente contenido:\n",
    "\n",
    "[INICIO DEL ARCHIVO]\n",
    "{contenido}\n",
    "[FIN DEL ARCHIVO]\n",
    "\n",
    "Ahora, por favor responde lo siguiente:\n",
    "{pregunta}\n",
    "\"\"\"\n",
    "    respuesta = query_deepseek(prompt)\n",
    "    print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2612cb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basándome en el diccionario de datos proporcionado, aquí tienes recomendaciones de feature engineering organizadas por categorías:\n",
      "\n",
      "### 1. **Variables Categóricas (One-Hot Encoding/Label Encoding)**\n",
      "   - **Estacionalidad**: Las variables `*_Season` (presentes en casi todos los instrumentos) podrían convertirse en variables dummy (one-hot encoding) para capturar efectos estacionales.\n",
      "   - **Variables con etiquetas**: \n",
      "     - `Sex` (0/1), `*_Zone` (ej. `FGC_CU_Zone`), `BIA_Activity_Level_num`, `BIA_Frame_num`, `PCIAT_*` (ítems 1-20), y `computerinternet_hoursday` son categóricas ordinales o nominales. Usa:\n",
      "       - **One-hot encoding** si no hay orden (ej. estaciones).\n",
      "       - **Label encoding** si hay orden (ej. `BIA_Activity_Level_num`: 1=Very Light a 5=Exceptional).\n",
      "     - Para `PCIAT_Total`, considera crear categorías basadas en los umbrales de severidad proporcionados (None/Mild/Moderate/Severe).\n",
      "\n",
      "### 2. **Normalización/Estandarización**\n",
      "   - **Variables continuas**: `Age`, `BMI`, `Height`, `Weight`, medidas de BIA (ej. `BIA_BMC`, `BIA_BMR`), y puntuaciones totales (`PAQ_A_Total`, `PAQ_C_Total`, `SDS_Total_Raw`). \n",
      "     - Usa **StandardScaler** si la distribución es normal.\n",
      "     - Usa **MinMaxScaler** si los valores tienen rangos fijos (ej. porcentajes).\n",
      "\n",
      "### 3. **Creación de Nuevas Features**\n",
      "   - **Antropometría**:\n",
      "     - `BMI` ya está incluido, pero podrías crear `BMI_ZScore` (para niños/adolescentes, usando tablas de referencia por edad/sexo).\n",
      "     - Ratio `Waist-to-Height` (Waist_Circumference/Height) como indicador de salud metabólica.\n",
      "   - **Fitness**:\n",
      "     - Combina `Time_Mins` y `Time_Sec` en una sola variable: `Total_Time_Seconds`.\n",
      "     - Crea un *composite score* de fitness (ej. promedio de zonas saludables: `FGC_*_Zone`).\n",
      "   - **PCIAT**:\n",
      "     - Suma de ítems específicos para subescalas (ej. \"adicción\", \"impacto social\").\n",
      "   - **Sueño**:\n",
      "     - Clasifica `SDS_Total_T` en categorías basadas en puntuaciones clínicas (si existen).\n",
      "\n",
      "### 4. **Manejo de Valores Faltantes**\n",
      "   - **Instrumentos con muchas features**: BIA y PCIAT tienen múltiples columnas. Considera:\n",
      "     - **Imputación** por mediana/moda si los faltantes son <10%.\n",
      "     - **Indicadores de faltante** (ej. `BIA_missing_flag`) si son significativos.\n",
      "\n",
      "### 5. **Reducción de Dimensionalidad**\n",
      "   - **PCA/Análisis factorial**: Para cuestionarios con muchos ítems (ej. PCIAT con 20 ítems) o medidas de BIA (12+ variables correlacionadas).\n",
      "   - **Agregación**: Promedio de medidas repetidas (ej. `Sit & Reach` izquierdo/derecho).\n",
      "\n",
      "### 6. **Interacciones y No Linealidades**\n",
      "   - **Interacciones**: `Sex × Fitness_Score`, `Age × BMI`.\n",
      "   - **Binning**: Edad en grupos (niño/adolescente) o BMI en categorías (bajo peso, normal, sobrepeso).\n",
      "\n",
      "### 7. **Temporalidad (si aplica)**\n",
      "   - Si hay múltiples registros por participante, crear features como:\n",
      "     - **Diferencias** entre temporadas (ej. cambio en `BMI` entre Spring y Fall).\n",
      "     - **Estabilidad** (ej. número de veces en \"Healthy Fitness Zone\").\n",
      "\n",
      "### Ejemplo de Código (Python):\n",
      "```python\n",
      "import pandas as pd\n",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
      "\n",
      "# Ejemplo para estacionalidad:\n",
      "df['Season_Spring'] = df['Basic_Demos-Enroll_Season'].apply(lambda x: 1 if x == 'Spring' else 0)\n",
      "# Ejemplo para PCIAT categories:\n",
      "df['PCIAT_Severity'] = pd.cut(df['PCIAT-PCIAT_Total'], bins=[0, 30, 49, 79, 100], labels=['None', 'Mild', 'Moderate', 'Severe'])\n",
      "# Escalado:\n",
      "scaler = StandardScaler()\n",
      "df[['Age', 'BMI']] = scaler.fit_transform(df[['Age', 'BMI']])\n",
      "```\n",
      "\n",
      "### Consideraciones Finales:\n",
      "- **Contexto clínico**: Valida las transformaciones con expertos (ej. categorías de BMI para niños).\n",
      "- **Evaluación**: Prueba el impacto de cada feature en el modelo (usando técnicas como SHAP).\n",
      "\n",
      "¿Hay alguna área específica (ej. antropometría, cuestionarios) en la que quieras profundizar?\n"
     ]
    }
   ],
   "source": [
    "analizar_archivo(\"data\\data_dictionary.csv\", \"¿Puedes revisar este archivo y recomendarme su feature engineering?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b0b84",
   "metadata": {},
   "source": [
    "Hacer feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cdc77a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureProcessor:\n",
    "    def __init__(self):\n",
    "        self.encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        self.text_cols = []\n",
    "        self.cat_cols = []\n",
    "        self.model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "        self.embed_cols = []\n",
    "        \n",
    "    def preprocess(self, train, test):\n",
    "        \"\"\"Versión optimizada que evita fragmentación\"\"\"\n",
    "        try:\n",
    "            # 1. Identificar columnas seguras\n",
    "            self._identify_safe_columns(train, test)\n",
    "            \n",
    "            # 2. Procesar texto\n",
    "            if self.text_cols:\n",
    "                train, test = self._process_text_optimized(train, test)\n",
    "            \n",
    "            # 3. Codificar categóricas\n",
    "            if self.cat_cols:\n",
    "                train, test = self._encode_categoricals(train, test)\n",
    "            \n",
    "            # 4. Normalizar numéricas\n",
    "            train, test = self._scale_numerical(train, test)\n",
    "            \n",
    "            return train, test\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en preprocesamiento: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _identify_safe_columns(self, train, test):\n",
    "        \"\"\"Identifica columnas existentes en ambos datasets\"\"\"\n",
    "        common_cols = list(set(train.columns) & set(test.columns))\n",
    "        \n",
    "        self.text_cols = [\n",
    "            col for col in common_cols \n",
    "            if train[col].dtype == 'object' \n",
    "            and train[col].str.contains('[a-zA-Z]', regex=True, na=False).any()\n",
    "        ]\n",
    "        \n",
    "        self.cat_cols = [\n",
    "            col for col in common_cols \n",
    "            if train[col].dtype == 'object' \n",
    "            and col not in self.text_cols\n",
    "        ]\n",
    "    \n",
    "    def _process_text_optimized(self, train, test):\n",
    "        \"\"\"Procesamiento de texto sin fragmentación\"\"\"\n",
    "        # Generar todos los embeddings primero\n",
    "        train_embeddings = []\n",
    "        test_embeddings = []\n",
    "        \n",
    "        for col in self.text_cols:\n",
    "            train_text = train[col].fillna('').astype(str)\n",
    "            test_text = test[col].fillna('').astype(str)\n",
    "            \n",
    "            # Embeddings para train y test\n",
    "            train_emb = self.model.encode(train_text.tolist(), show_progress_bar=False)\n",
    "            test_emb = self.model.encode(test_text.tolist(), show_progress_bar=False)\n",
    "            \n",
    "            train_embeddings.append(train_emb)\n",
    "            test_embeddings.append(test_emb)\n",
    "        \n",
    "        # Concatenar todos los embeddings horizontalmente\n",
    "        if train_embeddings:\n",
    "            train_embeddings = np.hstack(train_embeddings)\n",
    "            test_embeddings = np.hstack(test_embeddings)\n",
    "            \n",
    "            # Crear DataFrames completos antes de asignar\n",
    "            n_features = train_embeddings.shape[1]\n",
    "            self.embed_cols = [f'text_embed_{i}' for i in range(n_features)]\n",
    "            \n",
    "            train_emb_df = pd.DataFrame(train_embeddings, columns=self.embed_cols, index=train.index)\n",
    "            test_emb_df = pd.DataFrame(test_embeddings, columns=self.embed_cols, index=test.index)\n",
    "            \n",
    "            # Concatenar de una sola vez\n",
    "            train = pd.concat([train, train_emb_df], axis=1)\n",
    "            test = pd.concat([test, test_emb_df], axis=1)\n",
    "            \n",
    "        return train, test\n",
    "    \n",
    "    def _encode_categoricals(self, train, test):\n",
    "        \"\"\"Codificación segura de categóricas\"\"\"\n",
    "        if self.cat_cols:\n",
    "            train_cats = train[self.cat_cols]\n",
    "            test_cats = test[self.cat_cols]\n",
    "            \n",
    "            # Codificar y asignar de una vez\n",
    "            train[self.cat_cols] = self.encoder.fit_transform(train_cats)\n",
    "            test[self.cat_cols] = self.encoder.transform(test_cats)\n",
    "            \n",
    "        return train, test\n",
    "    \n",
    "    def _scale_numerical(self, train, test):\n",
    "        \"\"\"Normalización optimizada\"\"\"\n",
    "        num_cols = [col for col in train.select_dtypes(include=np.number).columns \n",
    "                   if col not in ['Subject_ID', 'PCIAT-PCIAT_Total'] and col in test.columns]\n",
    "        \n",
    "        if num_cols:\n",
    "            means = train[num_cols].mean()\n",
    "            stds = train[num_cols].std() + 1e-8\n",
    "            \n",
    "            train[num_cols] = (train[num_cols] - means) / stds\n",
    "            test[num_cols] = (test[num_cols] - means) / stds\n",
    "            \n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f23643ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_actigraphy(df, subject_id_col='Subject_ID'):\n",
    "    \"\"\"\n",
    "    Enhanced actigraphy processing with:\n",
    "    - Percentiles (10th, 25th, 75th, 90th)\n",
    "    - Robust statistical measures (IQR, MAD)\n",
    "    - Frequency domain features (FFT)\n",
    "    \"\"\"\n",
    "    exclude_cols = [subject_id_col, 'timestamp']\n",
    "    num_cols = [col for col in df.columns \n",
    "               if col not in exclude_cols \n",
    "               and pd.api.types.is_numeric_dtype(df[col])]\n",
    "    \n",
    "    # Time-domain features\n",
    "    stats = {\n",
    "        'mean': np.mean,\n",
    "        'std': np.std,\n",
    "        'min': np.min,\n",
    "        'max': np.max,\n",
    "        'median': np.median,\n",
    "        'skew': skew,\n",
    "        'kurtosis': kurtosis,\n",
    "        'q1': lambda x: np.percentile(x, 25),\n",
    "        'q3': lambda x: np.percentile(x, 75),\n",
    "        'iqr': lambda x: np.percentile(x, 75) - np.percentile(x, 25),\n",
    "        'mad': lambda x: np.median(np.abs(x - np.median(x)))\n",
    "    }\n",
    "    \n",
    "    # Frequency-domain features (simplified FFT)\n",
    "    def dominant_freq(x):\n",
    "        if len(x) < 2: return 0\n",
    "        fft = np.abs(np.fft.fft(x))\n",
    "        return np.argmax(fft[1:len(fft)//2]) + 1\n",
    "    \n",
    "    summary = df.groupby(subject_id_col)[num_cols].agg(stats)\n",
    "    summary.columns = [f'{col}_{stat}' for col, stat in summary.columns]\n",
    "    \n",
    "    # Add frequency features\n",
    "    freq_features = df.groupby(subject_id_col)[num_cols].agg(dominant_freq)\n",
    "    freq_features.columns = [f'{col}_dominant_freq' for col in freq_features.columns]\n",
    "    \n",
    "    return pd.concat([summary, freq_features], axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f233bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_features(df, subject_col='Subject_ID'):\n",
    "    \"\"\"Extrae 15 features temporales clave por sujeto\"\"\"\n",
    "    features = []\n",
    "    for subject_id, group in df.groupby(subject_col):\n",
    "        if 'timestamp' in group.columns:\n",
    "            time_diff = group['timestamp'].diff().dt.total_seconds()\n",
    "            feat = {\n",
    "                'Subject_ID': subject_id,\n",
    "                'total_events': len(group),\n",
    "                'active_hours': (time_diff < 3600).sum(),\n",
    "                'night_activity': group[group['timestamp'].dt.hour.between(0, 6)]['value'].mean(),\n",
    "                'max_activity': group['value'].max(),\n",
    "                'std_activity': group['value'].std(),\n",
    "            }\n",
    "            features.append(feat)\n",
    "    return pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f6ed58",
   "metadata": {},
   "source": [
    "Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01c5d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X, y):\n",
    "    \"\"\"Versión optimizada sin warnings\"\"\"\n",
    "    model = XGBClassifier(\n",
    "        objective='multi:softmax',\n",
    "        num_class=len(np.unique(y)),\n",
    "        n_estimators=150,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=0.5,\n",
    "        tree_method='hist',  # Más eficiente\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    qwk_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=0\n",
    "        )\n",
    "        preds = model.predict(X_val)\n",
    "        qwk_scores.append(cohen_kappa_score(y_val, preds, weights='quadratic'))\n",
    "    \n",
    "    return model, np.mean(qwk_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7265a910",
   "metadata": {},
   "source": [
    "Complete the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58a2ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Utility functions for competition submission\n",
    "'''\n",
    "\n",
    "def save_submission(test, preds, output_dir):\n",
    "    '''\n",
    "    Saves predictions in Kaggle submission format\n",
    "    \n",
    "    Args:\n",
    "        test: Test DataFrame\n",
    "        preds: Model predictions\n",
    "        output_dir: Directory to save submission file\n",
    "    '''\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test['id'],\n",
    "        'sii': preds\n",
    "    })\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    submission_path = os.path.join(output_dir, 'submission.csv')\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "727e2827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cargando datos ===\n",
      "\n",
      "=== Preprocesamiento ===\n",
      "\n",
      "=== Preparando target ===\n",
      "\n",
      "=== Seleccionando features ===\n",
      "Features seleccionadas: 4272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leosa\\AppData\\Local\\Temp\\ipykernel_6368\\2534072281.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['SII_group'], bins = pd.qcut(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Entrenamiento ===\n",
      "\n",
      "✔ QWK promedio: 0.4285\n",
      "\n",
      "=== Generando submission ===\n",
      "Submission saved to C:\\TrabajoFinal\\Child_mind_institute_problematic_internet_use\\src\\outputs\\submission.csv\n",
      "✔ Submission generado en C:\\TrabajoFinal\\Child_mind_institute_problematic_internet_use\\src\\outputs\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try:\n",
    "        print(\"\\n=== Cargando datos ===\")\n",
    "        train = pd.read_csv(train_path)\n",
    "        test = pd.read_csv(test_path)\n",
    "        \n",
    "        # Verificación de datos críticos\n",
    "        assert 'PCIAT-PCIAT_Total' in train.columns, \"Columna target no encontrada\"\n",
    "        train = train.dropna(subset=['PCIAT-PCIAT_Total'])\n",
    "        \n",
    "        print(\"\\n=== Preprocesamiento ===\")\n",
    "        processor = FeatureProcessor()\n",
    "        train, test = processor.preprocess(train, test)\n",
    "        \n",
    "        print(\"\\n=== Preparando target ===\")\n",
    "        # Versión robusta de qcut\n",
    "        train['SII_group'], bins = pd.qcut(\n",
    "            train['PCIAT-PCIAT_Total'],\n",
    "            q=4,\n",
    "            labels=[0, 1, 2, 3],\n",
    "            retbins=True,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        y = train['SII_group'].astype(int)\n",
    "        \n",
    "        print(\"\\n=== Seleccionando features ===\")\n",
    "        # Excluir columnas no relevantes y asegurar consistencia\n",
    "        exclude = ['PCIAT-PCIAT_Total', 'Subject_ID', 'SII_group', 'timestamp', 'id']\n",
    "        \n",
    "        # Solo features presentes en ambos datasets\n",
    "        common_features = list(set(train.columns) & set(test.columns))\n",
    "        features = [\n",
    "            col for col in common_features\n",
    "            if col not in exclude\n",
    "            and pd.api.types.is_numeric_dtype(train[col])\n",
    "            and col in test.columns\n",
    "        ]\n",
    "        \n",
    "        print(f\"Features seleccionadas: {len(features)}\")\n",
    "        X = train[features]\n",
    "        \n",
    "        print(\"\\n=== Entrenamiento ===\")\n",
    "        model, qwk = train_and_evaluate(X, y)\n",
    "        print(f\"\\n✔ QWK promedio: {qwk:.4f}\")\n",
    "        \n",
    "        print(\"\\n=== Generando submission ===\")\n",
    "        # Verificar features en test\n",
    "        missing_in_test = [col for col in features if col not in test.columns]\n",
    "        if missing_in_test:\n",
    "            print(f\"⚠ Features faltantes en test: {missing_in_test}\")\n",
    "            features = [col for col in features if col in test.columns]\n",
    "        \n",
    "        test_preds = model.predict(test[features])\n",
    "        \n",
    "        # Asegurar ID para submission\n",
    "        if 'id' not in test.columns and 'Subject_ID' in test.columns:\n",
    "            test['id'] = test['Subject_ID']\n",
    "        elif 'id' not in test.columns:\n",
    "            test['id'] = range(len(test))\n",
    "        \n",
    "        save_submission(test, test_preds, OUTPUT_DIR)\n",
    "        print(f\"✔ Submission generado en {OUTPUT_DIR / 'submission.csv'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error crítico: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
