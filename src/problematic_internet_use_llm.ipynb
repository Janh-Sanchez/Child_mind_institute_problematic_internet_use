{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b738c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "de19fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define robust directory paths within the notebook environment.\n",
    "BASE_DIR = Path().resolve()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "\n",
    "train_path = DATA_DIR / 'train.csv'\n",
    "test_path = DATA_DIR / 'test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a961ffb",
   "metadata": {},
   "source": [
    "Importacion de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5594db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "'# Load environment variables from .env file'\n",
    "def load_data(data_dir='data'):\n",
    "    try:\n",
    "        # Define robust paths for CSV and Parquet files\n",
    "        train_csv_path = os.path.join(data_dir, 'train.csv')\n",
    "        test_csv_path = os.path.join(data_dir, 'test.csv')\n",
    "        train_parquet_path = os.path.join(data_dir, 'train.parquet')\n",
    "        test_parquet_path = os.path.join(data_dir, 'test.parquet')\n",
    "        \n",
    "        if not all(os.path.exists(f) for f in [train_csv_path, test_csv_path]):\n",
    "            raise FileNotFoundError(\"CSV files not found in the ‘data’ folder.\")\n",
    "        \n",
    "        train_csv = pd.read_csv(train_csv_path)\n",
    "        test_csv = pd.read_csv(test_csv_path)\n",
    "        \n",
    "        train_parquet = pd.DataFrame()\n",
    "        test_parquet = pd.DataFrame()\n",
    "        if os.path.exists(train_parquet_path):\n",
    "            train_parquet = pq.read_table(train_parquet_path).to_pandas()\n",
    "        if os.path.exists(test_parquet_path):\n",
    "            test_parquet = pq.read_table(test_parquet_path).to_pandas()\n",
    "        \n",
    "        # Merge CSV and Parquet data on 'Subject_ID'\n",
    "        train = pd.merge(train_csv, train_parquet, on='Subject_ID', how='left') if not train_parquet.empty else train_csv\n",
    "        test = pd.merge(test_csv, test_parquet, on='Subject_ID', how='left') if not test_parquet.empty else test_csv\n",
    "        \n",
    "        return train, test, {}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf01c1",
   "metadata": {},
   "source": [
    "LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "57b5ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # Load environment variables from .env file\n",
    "\n",
    "api_key = os.getenv(\"api1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4acffd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"api1\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")# Initialize OpenAI client with API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "efc004d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'# Function to query DeepSeek model with a prompt'\n",
    "def query_deepseek(prompt: str, model: str = \"deepseek/deepseek-r1:free\") -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\":\"user\", \"content\":prompt}],\n",
    "        temperature=0.6,\n",
    "        max_tokens=4096,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a03c557",
   "metadata": {},
   "source": [
    "Prompts que hay que realizar de manera general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ee2d5864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "\"\"\"\n",
      "Loads and analyzes cells from a Jupyter notebook, focusing on code content.\n",
      "Changes from original:\n",
      "- Added type hints for clarity\n",
      "- Improved variable names for readability\n",
      "- Added docstring and comments\n",
      "- Maintained consistent quoting style\n",
      "- Removed redundant parameters\n",
      "\"\"\"\n",
      "\n",
      "# Core functionality remains unchanged - notebook analysis without cell outputs\n",
      "from langchain_community.document_loaders import NotebookLoader\n",
      "\n",
      "# Initialize loader with explicit parameter names for better readability\n",
      "loader: NotebookLoader = NotebookLoader(\n",
      "    \"prueba-llm.ipynb\",\n",
      "    include_outputs=False,  # Exclude cell outputs per original functionality\n",
      "    remove_newline=True     # Maintain text compactness as in original\n",
      ")\n",
      "\n",
      "# Load notebook cells with descriptive variable name\n",
      "notebook_cells: list = loader.load()\n",
      "\n",
      "# Output analysis result matching original functionality\n",
      "print(f\"Cells found: {len(notebook_cells)}\")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "original_code = \"\"\"\n",
    "# Load the active notebook (you can see its name in Kaggle)\n",
    "from langchain_community.document_loaders import NotebookLoader\n",
    "\n",
    "loader = NotebookLoader(\"prueba-llm.ipynb\", include_outputs=False, remove_newline=True)\n",
    "docs = loader.load()\n",
    "print(f\"Cells found: {len(docs)}\")\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Rewrite the following code with style and efficiency improvements, without changing its functionality:\\n{original_code},\n",
    "also, everything that is not related to the code should be commented, including the changes made\"\"\"\n",
    "\n",
    "response = query_deepseek(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b8a36b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'# Function to analyze a file and answer a question using DeepSeek'\n",
    "def analyze_file(filepath: str, question: str):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    prompt = f\"\"\"I have this file named {filepath} with the following content:\n",
    "\n",
    "[START OF FILE]\n",
    "{content}\n",
    "[END OF FILE]\n",
    "\n",
    "Now, please answer the following:\n",
    "{question}\n",
    "\"\"\"\n",
    "    response = query_deepseek(prompt)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2612cb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are key feature engineering recommendations for your dataset:\n",
      "\n",
      "1. **Categorical Encoding:**\n",
      "- One-hot encode multi-class categories (Seasons, BIA_Activity_Level, Frame_num)\n",
      "- Keep binary categoricals (Sex, fitness zones) as 0/1 indicators\n",
      "- Treat ordinal categories (computerinternet_hoursday) as ordered numericals\n",
      "\n",
      "2. **Temporal Features:**\n",
      "- Convert all seasonal columns to cyclical features using sine/cosine transforms\n",
      "- Create \"Season_Consistency\" flag if enrollment season matches participation season\n",
      "- Combine FitnessGram's Time_Mins + Time_Sec → Total_Seconds (continuous)\n",
      "\n",
      "3. **Biometric Interactions:**\n",
      "- Create Waist-to-Height Ratio: (Waist_Circumference/Height)\n",
      "- Add Grip Strength Asymmetry: (GSD - GSND)/GSD\n",
      "- Derive Pulse Pressure: Systolic_BP - Diastolic_BP\n",
      "\n",
      "4. **Composite Scores:**\n",
      "- Create PCIAT subscales:\n",
      "  - *Time Management* (Q1,Q5,Q10,Q16,Q18)\n",
      "  - *Social Impact* (Q3,Q8,Q19)\n",
      "  - *Academic/Occupational* (Q6,Q17)\n",
      "- Combine fitness test totals (Curl-ups + Push-ups + Trunk Lift) → Core_Strength_Score\n",
      "\n",
      "5. **Binning/Discretization:**\n",
      "- Create age groups (e.g., 6-11, 12-14, 15-18)\n",
      "- Bin heart rate/blood pressure using pediatric percentiles\n",
      "- Discretize internet usage hours into Low/Medium/High\n",
      "\n",
      "6. **Redundancy Handling:**\n",
      "- Investigate BMI differences between Physical Measures vs BIA measurements\n",
      "- Consider removing duplicate seasonal columns after creating unified seasonal features\n",
      "\n",
      "7. **Domain-Specific Features:**\n",
      "- Calculate FFMI (Fat Free Mass Index) using: (FFM/2.2)/((Height*0.0254)^2)\n",
      "- Create hydration ratio: TBW/(FFM+LST)\n",
      "- Derive activity energy ratio: DEE/BMR\n",
      "\n",
      "8. **Normalization:**\n",
      "- Z-score normalize continuous variables (Age, Height, Weight) for linear models\n",
      "- Min-max scale questionnaire totals (PCIAT_Total, SDS_Total_Raw) to [0,1]\n",
      "\n",
      "9. **Missing Data Strategy:**\n",
      "- Create missing indicators for BIA_Activity_Level and Frame_num (\"Unknown\" category)\n",
      "- Use KNN imputation for physiological measurements within seasonal groups\n",
      "\n",
      "10. **Feature Selection:**\n",
      "- Perform correlation analysis between BIA components (ECW/ICW/TBW)\n",
      "- Use mutual information for categorical-target relationships\n",
      "\n",
      "**Special Considerations:**\n",
      "- Keep all PCIAT items while creating subscales rather than just using the total score\n",
      "- Maintain separate dominant/non-dominant grip measurements for asymmetry analysis\n",
      "- Preserve both raw and T-scores for SDS to capture different variance aspects\n",
      "\n",
      "Would you like me to elaborate on any particular aspect or provide implementation code for specific transformations?\n"
     ]
    }
   ],
   "source": [
    "analyze_file(\"data/data_dictionary.csv\", \"Can you review this file and recommend feature engineering steps?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1ba5802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To view the contents of a Parquet file, you can use various tools and methods depending on your needs and technical setup. Here are the most common approaches:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Using Python (Pandas/PyArrow)**\n",
      "**Prerequisites**: Install `pandas` and `pyarrow` (or `fastparquet`).\n",
      "```bash\n",
      "pip install pandas pyarrow\n",
      "```\n",
      "\n",
      "#### **Example Code**:\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Read the Parquet file into a DataFrame\n",
      "df = pd.read_parquet('data.parquet')\n",
      "\n",
      "# Display the data\n",
      "print(df.head())  # Show first 5 rows\n",
      "print(df.schema)  # Show column names and data types (if using PyArrow)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Command-Line Tools**\n",
      "#### **a. parquet-tools** (Apache Parquet CLI)\n",
      "Install via [PyPI](https://pypi.org/project/parquet-tools/):\n",
      "```bash\n",
      "pip install parquet-tools\n",
      "```\n",
      "\n",
      "**Commands**:\n",
      "```bash\n",
      "parquet-tools head file.parquet      # View first few rows\n",
      "parquet-tools schema file.parquet    # View schema (column names/types)\n",
      "parquet-tools meta file.parquet      # View metadata\n",
      "```\n",
      "\n",
      "#### **b. DuckDB** (SQL-based querying)\n",
      "Install DuckDB and use SQL to query Parquet files:\n",
      "```bash\n",
      "duckdb\n",
      "```\n",
      "```sql\n",
      "SELECT * FROM 'data.parquet';\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Big Data Tools**\n",
      "#### **Apache Spark**\n",
      "```python\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "spark = SparkSession.builder.appName(\"ParquetViewer\").getOrCreate()\n",
      "df = spark.read.parquet('data.parquet')\n",
      "df.show(5)  # Display first 5 rows\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **4. GUI Tools**\n",
      "- **DBeaver** (Database GUI): Connect to Parquet as a data source.\n",
      "- **VS Code Extensions**: Use plugins like *Parquet Viewer*.\n",
      "- **Online Viewers**: \n",
      "  - [Parquet Online Viewer](https://parquet-viewer.com/) (upload files securely).\n",
      "\n",
      "---\n",
      "\n",
      "### **5. Quick Preview in Jupyter Notebook**\n",
      "Use PyArrow to inspect metadata without loading the entire file:\n",
      "```python\n",
      "import pyarrow.parquet as pq\n",
      "\n",
      "table = pq.read_table('data.parquet')\n",
      "print(table.schema)\n",
      "print(table.to_pandas().head())\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Notes**:\n",
      "- For **partitioned datasets**, specify the directory path instead of a single file.\n",
      "- Ensure the file is not corrupted or encrypted.\n",
      "- Avoid sharing sensitive data via online tools.\n",
      "\n",
      "Choose the method that best fits your workflow! 🚀\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "How to see .parquet data\n",
    "\"\"\"\n",
    "\n",
    "response = query_deepseek(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b0b84",
   "metadata": {},
   "source": [
    "Hacer feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b9c69be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder, \n",
    "    OrdinalEncoder, \n",
    "    StandardScaler, \n",
    "    MinMaxScaler,\n",
    "    KBinsDiscretizer\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing pipeline implementing all feature engineering recommendations\n",
    "    \"\"\"\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "\n",
    "    ## 0. Temporal Sorting (antes de eliminar columnas como 'Season')\n",
    "    season_backup = None\n",
    "    if 'id' in df_processed.columns and 'Season' in df_processed.columns:\n",
    "        season_backup = df_processed['Season'].copy()\n",
    "        df_processed = df_processed.sort_values(['id', 'Season'])\n",
    "\n",
    "    ## 1. Categorical Variable Encoding\n",
    "    binary_cols = ['Sex', 'FGC_CU_Zone']  # Add other binary columns\n",
    "    for col in binary_cols:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed[col] = df_processed[col].astype(int)\n",
    "\n",
    "    # One-hot encoding for nominal categories\n",
    "    nominal_cols = ['Season']  # Add other nominal columns\n",
    "    ohe = OneHotEncoder(drop='first', sparse_output=False)\n",
    "    for col in nominal_cols:\n",
    "        if col in df_processed.columns:\n",
    "            encoded = ohe.fit_transform(df_processed[[col]])\n",
    "            encoded_df = pd.DataFrame(encoded, columns=ohe.get_feature_names_out([col]), index=df_processed.index)\n",
    "            df_processed = pd.concat([df_processed.drop(col, axis=1), encoded_df], axis=1)\n",
    "\n",
    "    # Ordinal encoding for ordered categories\n",
    "    ordinal_mappings = {\n",
    "        'BIA_Activity_Level_num': [1, 2, 3, 4, 5],  # 1=Very Light to 5=Exceptional\n",
    "        'computerinternet_hoursday': [0, 1, 2, 3]    # 0=Less than 1h/day to 3=More than 3hs/day\n",
    "    }\n",
    "    ordinal_encoder = OrdinalEncoder(categories=[ordinal_mappings[col] for col in ordinal_mappings.keys()])\n",
    "    for col in ordinal_mappings:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed[col] = ordinal_encoder.fit_transform(df_processed[[col]])\n",
    "\n",
    "    ## 2. Feature Transformation\n",
    "    if season_backup is not None:\n",
    "        seasons = {'Spring': 0, 'Summer': 1, 'Fall': 2, 'Winter': 3}\n",
    "        df_processed['Season_sin'] = season_backup.map(seasons).apply(lambda x: np.sin(x * (2*np.pi/4)))\n",
    "        df_processed['Season_cos'] = season_backup.map(seasons).apply(lambda x: np.cos(x * (2*np.pi/4)))\n",
    "\n",
    "    skewed_cols = ['BIA_BMR', 'BIA_DEE', 'PAQ_A_Total']\n",
    "    for col in skewed_cols:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed[f'log_{col}'] = np.log1p(df_processed[col])\n",
    "\n",
    "    if 'Age' in df_processed.columns:\n",
    "        binner = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "        df_processed['Age_bin'] = binner.fit_transform(df_processed[['Age']])\n",
    "\n",
    "    ## 3. Composite Features\n",
    "    if all(col in df_processed.columns for col in ['Fitness_Endurance-Time_Mins', 'Time_Sec']):\n",
    "        df_processed['Total_Time_Seconds'] = df_processed['Fitness_Endurance-Time_Mins'] * 60 + df_processed['Time_Sec']\n",
    "\n",
    "    if all(col in df_processed.columns for col in ['FGC_GSD', 'FGC_GSND']):\n",
    "        df_processed['Grip_Strength_Asymmetry'] = (df_processed['FGC_GSD'] - df_processed['FGC_GSND']) / \\\n",
    "                                                  (df_processed['FGC_GSD'] + df_processed['FGC_GSND'] + 1e-6)\n",
    "\n",
    "    if all(col in df_processed.columns for col in ['BMI', 'BIA_Fat']):\n",
    "        df_processed['BMI_Fat_Interaction'] = df_processed['BMI'] * df_processed['BIA_Fat']\n",
    "\n",
    "    ## 4. Domain-Specific Aggregations\n",
    "    fitness_cols = ['FGC_CU', 'FGC_PU', 'FGC_TL']\n",
    "    if all(col in df_processed.columns for col in fitness_cols):\n",
    "        df_processed['Fitness_Composite'] = df_processed[fitness_cols].mean(axis=1)\n",
    "\n",
    "    if all(col in df_processed.columns for col in ['BIA_Fat_Mass', 'Height']):\n",
    "        df_processed['FMI'] = df_processed['BIA_Fat_Mass'] / (df_processed['Height'] / 100)**2\n",
    "\n",
    "    ## 5. Handling Missing Data\n",
    "    cols_with_missing = df_processed.columns[df_processed.isnull().any()].tolist()\n",
    "    for col in cols_with_missing:\n",
    "        df_processed[f'Missing_{col}'] = df_processed[col].isnull().astype(int)\n",
    "\n",
    "    numeric_cols = df_processed.select_dtypes(include=['number']).columns\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df_processed[numeric_cols] = imputer.fit_transform(df_processed[numeric_cols])\n",
    "\n",
    "    ## 6. Normalization/Scaling\n",
    "    to_standard_scale = ['Height', 'Weight']\n",
    "    scaler = StandardScaler()\n",
    "    for col in to_standard_scale:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed[f'{col}_scaled'] = scaler.fit_transform(df_processed[[col]])\n",
    "\n",
    "    to_minmax_scale = ['CGAS_Score']\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    for col in to_minmax_scale:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed[f'{col}_scaled'] = minmax_scaler.fit_transform(df_processed[[col]])\n",
    "\n",
    "    ## 7. Target-Specific Engineering\n",
    "    if all(col in df_processed.columns for col in ['Systolic_BP', 'Diastolic_BP', 'HeartRate']):\n",
    "        df_processed['Cardio_Risk_Score'] = (\n",
    "            df_processed['Systolic_BP'] / 140 + \n",
    "            df_processed['Diastolic_BP'] / 90 + \n",
    "            df_processed['HeartRate'] / 100\n",
    "        )\n",
    "\n",
    "    ## 8. Dimensionality Reduction\n",
    "    correlated_groups = [\n",
    "        ['BIA_TBW', 'BIA_ECW', 'BIA_ICW'],\n",
    "        ['SDS_Total_Raw', 'SDS_Total_T']\n",
    "    ]\n",
    "    for group in correlated_groups:\n",
    "        if all(col in df_processed.columns for col in group):\n",
    "            pca = PCA(n_components=1)\n",
    "            pca_feature = pca.fit_transform(df_processed[group])\n",
    "            group_name = '_'.join(group)\n",
    "            df_processed[f'{group_name}_PCA'] = pca_feature\n",
    "\n",
    "    ## 9. Temporal Feature Derivatives\n",
    "    if 'id' in df_processed.columns:\n",
    "        change_cols = ['BMI', 'Fitness_Composite']\n",
    "        for col in change_cols:\n",
    "            if col in df_processed.columns:\n",
    "                df_processed[f'Delta_{col}'] = df_processed.groupby('id')[col].diff()\n",
    "\n",
    "    ## 10. Textual Data\n",
    "    if 'Description' in df_processed.columns:\n",
    "        df_processed['Dominant_Mentioned'] = df_processed['Description'].str.contains('dominant', case=False).astype(int)\n",
    "\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cdc77a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "' FeatureProcessor class for preprocessing data with text and categorical features'\n",
    "class FeatureProcessor:\n",
    "    def __init__(self):\n",
    "        self.encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        self.text_cols = []\n",
    "        self.cat_cols = []\n",
    "        self.model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "        self.embed_cols = []\n",
    "        \n",
    "    def preprocess(self, train, test):\n",
    "        'Optimized preprocessing to avoid fragmentation'\n",
    "        try:\n",
    "            self._identify_safe_columns(train, test)\n",
    "            \n",
    "            if self.text_cols:\n",
    "                train, test = self._process_text_optimized(train, test)\n",
    "            \n",
    "            if self.cat_cols:\n",
    "                train, test = self._encode_categoricals(train, test)\n",
    "            # normalization\n",
    "            train, test = self._scale_numerical(train, test)\n",
    "            \n",
    "            return train, test\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error on preprocesing: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _identify_safe_columns(self, train, test):\n",
    "        \"\"\"Identifica columnas existentes en ambos datasets\"\"\"\n",
    "        common_cols = list(set(train.columns) & set(test.columns))\n",
    "        \n",
    "        self.text_cols = [\n",
    "            col for col in common_cols \n",
    "            if train[col].dtype == 'object' \n",
    "            and train[col].str.contains('[a-zA-Z]', regex=True, na=False).any()\n",
    "        ]\n",
    "        \n",
    "        self.cat_cols = [\n",
    "            col for col in common_cols \n",
    "            if train[col].dtype == 'object' \n",
    "            and col not in self.text_cols\n",
    "        ]\n",
    "    \n",
    "    def _process_text_optimized(self, train, test):\n",
    "        'procesing of text without fragmentation'\n",
    "        train_embeddings = []\n",
    "        test_embeddings = []\n",
    "        \n",
    "        for col in self.text_cols:\n",
    "            train_text = train[col].fillna('').astype(str)\n",
    "            test_text = test[col].fillna('').astype(str)\n",
    "            \n",
    "            # Embeddings for train and test\n",
    "            train_emb = self.model.encode(train_text.tolist(), show_progress_bar=False)\n",
    "            test_emb = self.model.encode(test_text.tolist(), show_progress_bar=False)\n",
    "            \n",
    "            train_embeddings.append(train_emb)\n",
    "            test_embeddings.append(test_emb)\n",
    "        \n",
    "        # concatenate all embeddings horizontally\n",
    "        if train_embeddings:\n",
    "            train_embeddings = np.hstack(train_embeddings)\n",
    "            test_embeddings = np.hstack(test_embeddings)\n",
    "            \n",
    "            # Create dataframes before assigning\n",
    "            n_features = train_embeddings.shape[1]\n",
    "            self.embed_cols = [f'text_embed_{i}' for i in range(n_features)]\n",
    "            \n",
    "            train_emb_df = pd.DataFrame(train_embeddings, columns=self.embed_cols, index=train.index)\n",
    "            test_emb_df = pd.DataFrame(test_embeddings, columns=self.embed_cols, index=test.index)\n",
    "            \n",
    "            # Concatenate embeddings\n",
    "            train = pd.concat([train, train_emb_df], axis=1)\n",
    "            test = pd.concat([test, test_emb_df], axis=1)\n",
    "            \n",
    "        return train, test\n",
    "    \n",
    "    def _encode_categoricals(self, train, test):\n",
    "        \"\"\"codification of categoricals\"\"\"\n",
    "        if self.cat_cols:\n",
    "            train_cats = train[self.cat_cols]\n",
    "            test_cats = test[self.cat_cols]\n",
    "            \n",
    "            # Ensure both datasets have the same categories\n",
    "            train[self.cat_cols] = self.encoder.fit_transform(train_cats)\n",
    "            test[self.cat_cols] = self.encoder.transform(test_cats)\n",
    "            \n",
    "        return train, test\n",
    "    \n",
    "    def _scale_numerical(self, train, test):\n",
    "        num_cols = [col for col in train.select_dtypes(include=np.number).columns \n",
    "                   if col not in ['Subject_ID', 'PCIAT-PCIAT_Total'] and col in test.columns]\n",
    "        \n",
    "        if num_cols:\n",
    "            means = train[num_cols].mean()\n",
    "            stds = train[num_cols].std() + 1e-8\n",
    "            \n",
    "            train[num_cols] = (train[num_cols] - means) / stds\n",
    "            test[num_cols] = (test[num_cols] - means) / stds\n",
    "            \n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f23643ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "' Summarize actigraphy data with enhanced statistical features'\n",
    "def summarize_actigraphy(df, subject_id_col='Subject_ID'):\n",
    "    \"\"\"\n",
    "    Enhanced actigraphy processing with:\n",
    "    - Percentiles (10th, 25th, 75th, 90th)\n",
    "    - Robust statistical measures (IQR, MAD)\n",
    "    - Frequency domain features (FFT)\n",
    "    \"\"\"\n",
    "    exclude_cols = [subject_id_col, 'timestamp']\n",
    "    num_cols = [col for col in df.columns \n",
    "               if col not in exclude_cols \n",
    "               and pd.api.types.is_numeric_dtype(df[col])]\n",
    "    \n",
    "    # Time-domain features\n",
    "    stats = {\n",
    "        'mean': np.mean,\n",
    "        'std': np.std,\n",
    "        'min': np.min,\n",
    "        'max': np.max,\n",
    "        'median': np.median,\n",
    "        'skew': skew,\n",
    "        'kurtosis': kurtosis,\n",
    "        'q1': lambda x: np.percentile(x, 25),\n",
    "        'q3': lambda x: np.percentile(x, 75),\n",
    "        'iqr': lambda x: np.percentile(x, 75) - np.percentile(x, 25),\n",
    "        'mad': lambda x: np.median(np.abs(x - np.median(x)))\n",
    "    }\n",
    "    \n",
    "    # Frequency-domain features (simplified FFT)\n",
    "    def dominant_freq(x):\n",
    "        if len(x) < 2: return 0\n",
    "        fft = np.abs(np.fft.fft(x))\n",
    "        return np.argmax(fft[1:len(fft)//2]) + 1\n",
    "    \n",
    "    summary = df.groupby(subject_id_col)[num_cols].agg(stats)\n",
    "    summary.columns = [f'{col}_{stat}' for col, stat in summary.columns]\n",
    "    \n",
    "    # Add frequency features\n",
    "    freq_features = df.groupby(subject_id_col)[num_cols].agg(dominant_freq)\n",
    "    freq_features.columns = [f'{col}_dominant_freq' for col in freq_features.columns]\n",
    "    \n",
    "    return pd.concat([summary, freq_features], axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1f233bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_features(df, subject_col='Subject_ID'):\n",
    "    \"\"\"Extracts 15 key time features per subject\"\"\"\n",
    "    features = []\n",
    "    for subject_id, group in df.groupby(subject_col):\n",
    "        if 'timestamp' in group.columns:\n",
    "            time_diff = group['timestamp'].diff().dt.total_seconds()\n",
    "            feat = {\n",
    "                'Subject_ID': subject_id,\n",
    "                'total_events': len(group),\n",
    "                'active_hours': (time_diff < 3600).sum(),\n",
    "                'night_activity': group[group['timestamp'].dt.hour.between(0, 6)]['value'].mean(),\n",
    "                'max_activity': group['value'].max(),\n",
    "                'std_activity': group['value'].std(),\n",
    "            }\n",
    "            features.append(feat)\n",
    "    return pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f6ed58",
   "metadata": {},
   "source": [
    "Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "01c5d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "' Train and evaluate the model with optimized parameters'\n",
    "def train_and_evaluate(X, y):\n",
    "    model = XGBClassifier(\n",
    "        objective='multi:softmax',\n",
    "        num_class=len(np.unique(y)),\n",
    "        n_estimators=150,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=0.5,\n",
    "        tree_method='hist',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    qwk_scores = []\n",
    "    \"\"\"    Stratified K-Fold cross-validation to ensure balanced class distribution\"\"\"\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        # Fit the model\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=0\n",
    "        )\n",
    "        preds = model.predict(X_val)\n",
    "        qwk_scores.append(cohen_kappa_score(y_val, preds, weights='quadratic'))\n",
    "    \n",
    "    return model, np.mean(qwk_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7265a910",
   "metadata": {},
   "source": [
    "Complete the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "58a2ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Utility functions for competition submission\n",
    "'''\n",
    "\n",
    "def save_submission(test, preds, output_dir):\n",
    "    '''\n",
    "    Saves predictions in Kaggle submission format\n",
    "    \n",
    "    Args:\n",
    "        test: Test DataFrame\n",
    "        preds: Model predictions\n",
    "        output_dir: Directory to save submission file\n",
    "    '''\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test['id'],\n",
    "        'sii': preds\n",
    "    })\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    submission_path = os.path.join(output_dir, 'submission.csv')\n",
    "    submission.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0919f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        print(\"\\n=== Loading data ===\")\n",
    "        train = pd.read_csv(train_path)\n",
    "        test = pd.read_csv(test_path)\n",
    "        \n",
    "        assert 'PCIAT-PCIAT_Total' in train.columns, \"Target column not found in train data\"\n",
    "        train = train.dropna(subset=['PCIAT-PCIAT_Total'])\n",
    "\n",
    "        print(\"\\n=== General Feature Engineering ===\")\n",
    "        train = preprocess_data(train)\n",
    "        test = preprocess_data(test)\n",
    "\n",
    "        print(\"\\n=== Advanced Preprocessing (Text + Categoricals) ===\")\n",
    "        processor = FeatureProcessor()\n",
    "        train, test = processor.preprocess(train, test)\n",
    "\n",
    "        print(\"\\n=== Preparing target ===\")\n",
    "        train['SII_group'], bins = pd.qcut(\n",
    "            train['PCIAT-PCIAT_Total'],\n",
    "            q=4,\n",
    "            labels=[0, 1, 2, 3],\n",
    "            retbins=True,\n",
    "            duplicates='drop'\n",
    "        )\n",
    "        y = train['SII_group'].astype(int)\n",
    "\n",
    "        print(\"\\n=== Selecting features ===\")\n",
    "        exclude = ['PCIAT-PCIAT_Total', 'Subject_ID', 'SII_group', 'timestamp', 'id']\n",
    "        common_features = list(set(train.columns) & set(test.columns))\n",
    "        features = [\n",
    "            col for col in common_features\n",
    "            if col not in exclude and pd.api.types.is_numeric_dtype(train[col])\n",
    "        ]\n",
    "\n",
    "        print(f\"Selected features: {len(features)}\")\n",
    "        X = train[features]\n",
    "\n",
    "        print(\"\\n=== Training ===\")\n",
    "        model, qwk = train_and_evaluate(X, y)\n",
    "        print(f\"\\n✔ QWK average: {qwk:.4f}\")\n",
    "\n",
    "        print(\"\\n=== Generating submission ===\")\n",
    "        if 'id' not in test.columns and 'Subject_ID' in test.columns:\n",
    "            test['id'] = test['Subject_ID']\n",
    "        elif 'id' not in test.columns:\n",
    "            test['id'] = range(len(test))\n",
    "\n",
    "        test_preds = model.predict(test[features])\n",
    "        save_submission(test, test_preds, OUTPUT_DIR)\n",
    "        print(f\"Submission generated in {OUTPUT_DIR / 'submission.csv'}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCritical error: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "caabfbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading data ===\n",
      "\n",
      "=== General Feature Engineering ===\n",
      "\n",
      "=== Advanced Preprocessing (Text + Categoricals) ===\n",
      "\n",
      "=== Preparing target ===\n",
      "\n",
      "=== Selecting features ===\n",
      "Selected features: 4327\n",
      "\n",
      "=== Training ===\n",
      "\n",
      "✔ QWK average: 0.4255\n",
      "\n",
      "=== Generating submission ===\n",
      "Submission saved to C:\\TrabajoFinal\\Child_mind_institute_problematic_internet_use\\src\\outputs\\submission.csv\n",
      "Submission generated in C:\\TrabajoFinal\\Child_mind_institute_problematic_internet_use\\src\\outputs\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
