{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c1104a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\TrabajoFinal\\Child_mind_institute_problematic_internet_use\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import scipy\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import time\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from scipy.optimize import minimize\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e87f181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def describe_x(df):\n",
    "    X = df['X']\n",
    "    return [\n",
    "        X.std(),\n",
    "    ]\n",
    "\n",
    "def describe_y(df):\n",
    "    Y = df['Y']\n",
    "    return [\n",
    "        Y.std(),\n",
    "    ]\n",
    "\n",
    "def describe_z(df):\n",
    "    Z = df['Z']\n",
    "    return [\n",
    "        Z.std(),  \n",
    "    ]\n",
    "\n",
    "def describe_enmo(df):\n",
    "    enmo = df['enmo']\n",
    "    return [\n",
    "        enmo.mean(),  \n",
    "    ]\n",
    "\n",
    "def describe_anglez(df):\n",
    "    anglez = df['anglez']\n",
    "    return [\n",
    "        anglez.std(),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "108f8898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light level thresholds (in lux)\n",
    "light_bins = [\n",
    "    (0, 5, 'Twilight'),\n",
    "    (5, 10, 'Minimal Street Lighting'),\n",
    "    (10, 50, 'Sunset'),\n",
    "    (50, 80, 'Family Living Room'),\n",
    "    (80, 100, 'Hallway'),\n",
    "    (100, 320, 'Very Dark Overcast Day'),\n",
    "    (320, 500, 'Office Lighting'),\n",
    "    (500, 1000, 'Sunrise/Sunset'),\n",
    "    (1000, 10000, 'Overcast Day'),\n",
    "    (10000, 25000, 'Full Daylight'),\n",
    "    (25000, 130000, 'Direct Sunlight')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84d99010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_light(light_value):\n",
    "    for low, high, label in light_bins:\n",
    "        if low <= light_value < high:\n",
    "            return label\n",
    "    return 'Unknown'\n",
    "\n",
    "def describe_light(df):\n",
    "    df['light_category'] = df['light'].apply(categorize_light)\n",
    "    light_categories = df['light_category'].value_counts(normalize=True).to_dict()\n",
    "    \n",
    "    features = [light_categories.get(label, 0) for _, _, label in light_bins]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59d1db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_inactivity_streaks(df, window_size=100, threshold=10, top_n=5):\n",
    "    rolling_cumsum = df['enmo'].rolling(window=window_size).sum()\n",
    "    inactive = rolling_cumsum <= threshold\n",
    "    \n",
    "    # Calculate streaks\n",
    "    streak_lengths = []\n",
    "    current_streak = 0\n",
    "    for is_inactive in inactive:\n",
    "        if is_inactive:\n",
    "            current_streak += 1\n",
    "        else:\n",
    "            if current_streak > 0:\n",
    "                streak_lengths.append(current_streak)\n",
    "            current_streak = 0\n",
    "    \n",
    "    # If the last streak is still active, add it\n",
    "    if current_streak > 0:\n",
    "        streak_lengths.append(current_streak)\n",
    "    \n",
    "    # Sort streaks in descending order and pick top N\n",
    "    streak_lengths = sorted(streak_lengths, reverse=True)[:top_n]\n",
    "    \n",
    "    # Pad with zeros if there are fewer than N streaks\n",
    "    streak_lengths += [0] * (top_n - len(streak_lengths))\n",
    "    return streak_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5144ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_activity_streaks(df, window_size=100, threshold=1, top_n=5):\n",
    "    # Calculate cumsum of enmo in the defined window\n",
    "    rolling_cumsum = df['enmo'].rolling(window=window_size).sum()\n",
    "    \n",
    "    # Identify active windows (cumsum > threshold)\n",
    "    active = rolling_cumsum > threshold\n",
    "    \n",
    "    # Calculate streaks\n",
    "    streak_lengths = []\n",
    "    current_streak = 0\n",
    "    for is_active in active:\n",
    "        if is_active:\n",
    "            current_streak += 1\n",
    "        else:\n",
    "            if current_streak > 0:\n",
    "                streak_lengths.append(current_streak)\n",
    "            current_streak = 0\n",
    "    \n",
    "    # If the last streak is still active, add it\n",
    "    if current_streak > 0:\n",
    "        streak_lengths.append(current_streak)\n",
    "    \n",
    "    # Sort streaks in descending order and pick top N\n",
    "    streak_lengths = sorted(streak_lengths, reverse=True)[:top_n]\n",
    "    \n",
    "    # Pad with zeros if there are fewer than N streaks\n",
    "    streak_lengths += [0] * (top_n - len(streak_lengths))\n",
    "    return streak_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6014be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop(['step'], axis=1, inplace=True)\n",
    "   \n",
    "    features = []\n",
    "    features.extend(describe_x(df))\n",
    "    features.extend(describe_y(df))\n",
    "    features.extend(describe_z(df))\n",
    "    features.extend(describe_enmo(df))\n",
    "    features.extend(describe_anglez(df))\n",
    "    features.extend(describe_light(df))  \n",
    "    \n",
    "    enmo_active_ratio = (df['enmo'] > 0).mean()\n",
    "    features.append(enmo_active_ratio)\n",
    "    features.extend(longest_inactivity_streaks(df, threshold=1))\n",
    "    features.extend(longest_activity_streaks(df, threshold=5))\n",
    "   \n",
    "    return np.array(features), filename.split('=')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5b16a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_time_series(path) -> pd.DataFrame:\n",
    "    # for kaggle folder\n",
    "    if os.path.isdir(path):\n",
    "        ids = os.listdir(path)\n",
    "        if not ids:\n",
    "            print(f\"La carpeta {path} está vacía.\")\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(tqdm(executor.map(lambda fname: process_file(fname, path), ids), total=len(ids)))\n",
    "        if results:\n",
    "            stats, indexes = zip(*results)\n",
    "            df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "            df['id'] = indexes\n",
    "            return df\n",
    "        else:\n",
    "            raise ValueError(f\"No se encontraron archivos para procesar en {path}\")\n",
    "    # for unique parquet file\n",
    "    elif os.path.isfile(path) and path.endswith('.parquet'):\n",
    "        print(f\"Leyendo archivo único: {path}\")\n",
    "        df = pd.read_parquet(path)\n",
    "        features = []\n",
    "        features.extend(describe_x(df))\n",
    "        features.extend(describe_y(df))\n",
    "        features.extend(describe_z(df))\n",
    "        features.extend(describe_enmo(df))\n",
    "        features.extend(describe_anglez(df))\n",
    "        features.extend(describe_light(df))\n",
    "        enmo_active_ratio = (df['enmo'] > 0).mean()\n",
    "        features.append(enmo_active_ratio)\n",
    "        features.extend(longest_inactivity_streaks(df, threshold=1))\n",
    "        features.extend(longest_activity_streaks(df, threshold=5))\n",
    "        # uncomment if you want to return the id\n",
    "        return pd.DataFrame([features], columns=[f\"stat_{i}\" for i in range(len(features))])\n",
    "    else:\n",
    "        raise ValueError(f\"Ruta no válida: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd74d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "\n",
    "    for col, (col_min, col_max) in min_max_dict.items():\n",
    "        df[col] = df[col].clip(lower=col_min, upper=col_max)\n",
    "\n",
    "    bins = [0, 6, 12, 18, 100]\n",
    "    labels = ['1 to 6', '7 to 12', '13 to 18', '19 to 100']\n",
    "    df['Age_Binned'] = pd.cut(df['Basic_Demos-Age'], bins=bins, labels=labels, right=True)\n",
    "    df['Age_Sex'] = df['Age_Binned'].astype(str) + '_' + df['Basic_Demos-Sex'].astype(str)\n",
    "    \n",
    "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    \n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "    \n",
    "    df['PreInt_FGC_CU_PU'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['FGC-FGC_CU'] * df['FGC-FGC_PU']\n",
    "    df['FGC_GSND_GSD_Age'] = df['FGC-FGC_GSND'] * df['FGC-FGC_GSD'] * df['Basic_Demos-Age']\n",
    "    df['SDS_Activity'] = df['BIA-BIA_Activity_Level_num'] * df['SDS-SDS_Total_T']\n",
    "    \n",
    "    df['CGasync_Score_Normalized'] = df['CGAS-CGAS_Score'] - df.groupby('Basic_Demos-Enroll_Season')['CGAS-CGAS_Score'].transform('mean')\n",
    "    df['Internet_Physical_Difference'] = df['PreInt_EduHx-computerinternet_hoursday'] - df['PAQ_A-PAQ_A_Total']\n",
    "   \n",
    "    df[df.select_dtypes(include='object').columns] = df.select_dtypes(include='object').astype('category')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5a9502c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [03:31<00:00,  4.70it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.55it/s]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"src/data/train.csv\")\n",
    "test = pd.read_csv(\"src/data/test.csv\")\n",
    "sample = pd.read_csv(\"src/data/sample_submission.csv\")\n",
    "\n",
    "train_ts = load_time_series(\"src/data/series_train.parquet\")\n",
    "test_ts = load_time_series(\"src/data/series_test.parquet\")\n",
    "\n",
    "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "# Normalización inicial\n",
    "numeric_cols = train[test.columns].select_dtypes(include='number').columns\n",
    "min_max_dict = {col: (train[col].min(), train[col].max()) for col in numeric_cols}\n",
    "\n",
    "train = feature_engineering(train)\n",
    "test = feature_engineering(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da87fead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name sentence-transformers/all-MiniLM-L6-v2. Creating a new one with mean pooling.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "llm_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54468f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función robusta para convertir una fila en texto\n",
    "def create_llm_text(df, columns):\n",
    "    valid_cols = [col for col in columns if col in df.columns]\n",
    "    def row_to_text(row):\n",
    "        parts = []\n",
    "        for col in valid_cols:\n",
    "            try:\n",
    "                val = row[col]\n",
    "                if pd.notna(val):\n",
    "                    parts.append(f\"{col}={str(val)}\")\n",
    "            except KeyError:\n",
    "                continue\n",
    "        return \". \".join(parts)\n",
    "    return df.apply(row_to_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40e44cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar columnas que contengan preguntas relevantes Y existan en ambos conjuntos\n",
    "llm_columns = [col for col in train.columns if (\"PCIAT\" in col or \"FGC\" in col or \"SDS\" in col) and col in test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd8a5781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 124/124 [00:50<00:00,  2.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# Crear texto desde las respuestas\n",
    "train[\"llm_text\"] = create_llm_text(train, llm_columns)\n",
    "test[\"llm_text\"] = create_llm_text(test, llm_columns)\n",
    "\n",
    "# Generar embeddings usando el LLM\n",
    "train_llm_embeddings = llm_model.encode(train[\"llm_text\"].tolist(), show_progress_bar=True)\n",
    "test_llm_embeddings = llm_model.encode(test[\"llm_text\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "# Convertir embeddings en DataFrame\n",
    "train_llm_df = pd.DataFrame(train_llm_embeddings, columns=[f\"llm_emb_{i}\" for i in range(train_llm_embeddings.shape[1])])\n",
    "test_llm_df = pd.DataFrame(test_llm_embeddings, columns=[f\"llm_emb_{i}\" for i in range(test_llm_embeddings.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "321dd76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar embeddings al dataset\n",
    "train = pd.concat([train.reset_index(drop=True), train_llm_df.reset_index(drop=True)], axis=1)\n",
    "test = pd.concat([test.reset_index(drop=True), test_llm_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Eliminar columnas no numéricas antes del modelado\n",
    "train = train.drop(columns=['llm_text'], errors='ignore')\n",
    "test = test.drop(columns=['llm_text'], errors='ignore')\n",
    "\n",
    "# Eliminar columnas innecesarias y preparar datos para entrenamiento\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d16f470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar que 'sii' no tenga valores nulos\n",
    "train = train.dropna(subset=['sii'])\n",
    "\n",
    "# Variables objetivo\n",
    "target = train['PCIAT-PCIAT_Total']\n",
    "sii_target = train['sii']\n",
    "\n",
    "# Igualar columnas de entrenamiento y prueba\n",
    "train = train[test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d89e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_pciat_to_sii(pciat_values):\n",
    "    return np.select(\n",
    "        [pciat_values <= 30, \n",
    "         (pciat_values > 30) & (pciat_values <= 49),\n",
    "         (pciat_values > 49) & (pciat_values <= 79),\n",
    "         pciat_values > 79],\n",
    "        [0, 1, 2, 3],\n",
    "        default=3  # For PCIAT values greater than 79\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faff56e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51cc9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa7a7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84ad9068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subset(df, target, subset_size=0.8):\n",
    "    df_subset = df.sample(frac=subset_size, random_state=42)\n",
    "    target_subset = target.loc[df_subset.index]\n",
    "    return df_subset, target_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85deced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_noise_injection(df, target, noise_level, subset_size=0.2):\n",
    "\n",
    "    # Select a subset of data for augmentation\n",
    "    df_subset, target_subset = select_subset(df, target, subset_size)\n",
    "\n",
    "    # Split numeric and non-numeric columns\n",
    "    numeric_cols = df_subset.select_dtypes(include=['float64', 'int64'])\n",
    "    non_numeric_cols = df_subset.select_dtypes(exclude=['float64', 'int64'])\n",
    "\n",
    "    # Impute missing values in numeric columns\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    numeric_imputed = pd.DataFrame(imputer.fit_transform(numeric_cols), \n",
    "                                   columns=numeric_cols.columns, \n",
    "                                   index=numeric_cols.index)\n",
    "\n",
    "    # Add noise to numeric columns\n",
    "    augmented_numeric = numeric_imputed\n",
    "    for col in augmented_numeric.columns:\n",
    "        std_dev = augmented_numeric[col].std()\n",
    "        if std_dev > 0:  # Add noise only if variability exists\n",
    "            noise = np.random.normal(0, noise_level * std_dev, size=len(augmented_numeric))\n",
    "            augmented_numeric[col] += noise\n",
    "\n",
    "    # Concatenate back with non-numeric columns (align rows)\n",
    "    augmented_df = pd.concat([augmented_numeric, non_numeric_cols], axis=1)\n",
    "\n",
    "    # Ensure the column order matches the original subset\n",
    "    augmented_df = augmented_df[df_subset.columns]\n",
    "    return augmented_df, target_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4861e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data_with_nans(X, target, threshold=0.1, subset_size=0.2):\n",
    "   \n",
    "    df_subset, target_subset = select_subset(X, target, subset_size)\n",
    "    X_augmented = df_subset.reset_index(drop=True).copy()\n",
    "    \n",
    "    # Identify columns that already contain NaN values\n",
    "    columns_with_nan = [col for col in X.columns if X[col].isna().sum() > 0]\n",
    "    \n",
    "    # Mask for non-NaN values in columns that contain NaNs\n",
    "    non_nan_mask = X_augmented[columns_with_nan].notna()\n",
    "    \n",
    "    # Randomly select which column to set to NaN (for each row) where there's a valid value\n",
    "    for col in columns_with_nan:\n",
    "        # Create a random mask for columns with valid values (non-NaN)\n",
    "        random_mask = np.random.rand(len(X_augmented)) < threshold  # Adjust probability as needed\n",
    "        \n",
    "        # Apply the mask to select rows and set that column's value to NaN\n",
    "        X_augmented.loc[random_mask, col] = np.nan\n",
    "    \n",
    "    return X_augmented, target_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90dbad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels=None):\n",
    "    y_true = y_true.astype(np.int32)\n",
    "    y_pred = y_pred.astype(np.int32)\n",
    "    \n",
    "    if labels is None:\n",
    "        labels = sorted(set(y_true))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap='Blues', values_format='d')\n",
    "\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dadad60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(params, X, y, sii_target, label='', save_models=True, pruning_callback=None, n_repeats=5, return_qwk=False):\n",
    "    features = X.columns\n",
    "    start_time = time.time()\n",
    "    oof = []\n",
    "    y_oof = []\n",
    "    qwk_list = []\n",
    "    model_list = []\n",
    "   \n",
    "    n = 0\n",
    "    for repeat in tqdm(range(n_repeats)):\n",
    "        random_seed = np.random.randint(0, 10000)  # Generate a random seed for each repeat\n",
    "        folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=repeat)\n",
    "        \n",
    "        for fold, (idx_tr, idx_va) in enumerate(folds.split(X, sii_target)):\n",
    "            params['random_seeds'] = n\n",
    "            set_global_seed(n)\n",
    "            X_tr = X.iloc[idx_tr]\n",
    "            X_va = X.iloc[idx_va]\n",
    "            y_tr = y.iloc[idx_tr]\n",
    "            y_va = y.iloc[idx_va]\n",
    "            \n",
    "            \n",
    "            nan_prone_columns = [\n",
    "                col for col in X_tr.columns \n",
    "                if X_tr[col].isna().any()  # Has NaNs\n",
    "            ]\n",
    "    \n",
    "            # Step 1: Perform augmentation on X_tr\n",
    "            nan_augmented, nan_aug_target = augment_data_with_nans(X_tr, target, threshold=1, subset_size=0.2)\n",
    "            noise_augmented, noise_aug_target = gaussian_noise_injection(X_tr, y_tr, noise_level=0.02, subset_size=0.5)\n",
    "    \n",
    "            X_tr_augmented = pd.concat(\n",
    "                [nan_augmented, noise_augmented, X_tr[y_tr>49], X_tr[y_tr>49], X_tr[y_tr>49], X_tr[y_tr>79]],\n",
    "                ignore_index=True).reset_index(drop=True)\n",
    "            \n",
    "            y_tr_augmented = pd.concat(\n",
    "                [nan_aug_target, noise_aug_target, y_tr[y_tr>49], y_tr[y_tr>49], y_tr[y_tr>49], y_tr[y_tr>79]],\n",
    "                ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "\n",
    "            X_tr_combined = pd.concat([X_tr, X_tr_augmented], ignore_index=True).reset_index(drop=True)\n",
    "            y_tr_combined = pd.concat([y_tr, y_tr_augmented], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "            shuffled_indices = np.random.permutation(X_tr_combined.index)\n",
    "            X_tr_combined = X_tr_combined.iloc[shuffled_indices].reset_index(drop=True)\n",
    "            y_tr_combined = y_tr_combined.iloc[shuffled_indices].reset_index(drop=True)\n",
    "\n",
    "            \n",
    "            dtrain = lgb.Dataset(X_tr_combined, label=y_tr_combined)\n",
    "            dvalid = lgb.Dataset(X_va, label=y_va)\n",
    "\n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                dtrain,\n",
    "                valid_sets=[dtrain, dvalid],\n",
    "                num_boost_round=params['n_estimators'],\n",
    "            )\n",
    "\n",
    "            y_pred = model.predict(X_va)\n",
    "\n",
    "            if save_models:\n",
    "                model_list.append(model)\n",
    "            oof.append(y_pred)\n",
    "            y_oof.append(y_va)\n",
    "            \n",
    "            n +=1\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    y_oof_actuals = np.concatenate(y_oof)\n",
    "    oof_preds = np.concatenate(oof)\n",
    "    \n",
    "    # Post-processing: Map predictions\n",
    "    y_oof_sii = map_pciat_to_sii(y_oof_actuals)\n",
    "    oof_sii = map_pciat_to_sii(oof_preds)\n",
    "\n",
    "  \n",
    "    qwk = cohen_kappa_score(y_oof_sii, oof_sii, weights='quadratic')\n",
    "    mse = ((y_oof_actuals - oof_preds)**2).mean()  \n",
    "    print(f\"Overall QWK: {qwk:.3f}, MSE: {mse:.3f}, Time: {int((time.time() - start_time) / 60)} min\")\n",
    "\n",
    "    # Optimize thresholds\n",
    "    threshold_optimizer = minimize(evaluate_predictions, \n",
    "                                   x0=[34, 49, 62], \n",
    "                                   args=(y_oof_sii, oof_preds), \n",
    "                                   method='Nelder-Mead')\n",
    "    \n",
    "    optimized_preds = threshold_Rounder(oof_preds, threshold_optimizer.x)\n",
    "    optimized_qwk = cohen_kappa_score(y_oof_sii, optimized_preds, weights='quadratic')\n",
    "    accuracy = (y_oof_sii==optimized_preds).astype(np.float32).mean()\n",
    "    print(f\"Optimized QWK: {optimized_qwk:.3f}, Accuracy: {accuracy:.3f}, Thresholds: {threshold_optimizer.x}\")\n",
    "    \n",
    "    plot_confusion_matrix(y_oof_sii, oof_sii)\n",
    "    \n",
    "    if save_models:\n",
    "        saved_models[label] = {'features': features, 'model_list': model_list}\n",
    "\n",
    "    return optimized_qwk, threshold_optimizer.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de8b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [04:05<05:43,  7.16s/it]"
     ]
    }
   ],
   "source": [
    "saved_models = {}\n",
    "results = []\n",
    "for i in range(1):\n",
    "    params = {'verbosity': -1,  'device': 'cpu', 'metric': 'mse', 'n_estimators':150, 'max_depth':5, 'max_bin': 15, 'boosting_type': 'gbdt', 'lambda_l1': 0.0012071403780584485, 'lambda_l2': 19.943477818207878, 'min_child_weight': 0.01586977190723854, 'learning_rate': 0.030512450456770007, 'num_leaves': 295, 'colsample_bytree': 0.8569995659929517, 'bagging_fraction': 0.587037100215173, 'feature_fraction': 0.8955475330753205, 'bagging_freq': 1}\n",
    "    qwk, qwk_thresholded = cross_validate_model(params, train, target, sii_target, label='trial', save_models=True, n_repeats=100)\n",
    "    print(qwk)\n",
    "    results.append(qwk)\n",
    "print(f\"'mean {np.mean(results)}\")\n",
    "print(f\"diff {max(results) - min(results)}\")\n",
    "\n",
    "pred = [model.predict(test)  for model in saved_models['trial']['model_list']]\n",
    "\n",
    "n = 16\n",
    "i = 500\n",
    "plt.hist(np.array(pred)[:, n][:i], bins=30, alpha=0.7)\n",
    "\n",
    "# Get the mode\n",
    "mode_val = stats.mode(np.array(pred)[:, n][:i].round())[0]  # mode.value[0]\n",
    "\n",
    "# Overlay the mode on the histogram\n",
    "plt.axvline(mode_val, color='k', linestyle='dashed', linewidth=2, label=f'Mode: {mode_val}')\n",
    "plt.axvline(np.array(pred)[:, n][:i].mean(), color='r', linestyle='dashed', linewidth=2, label=f'mean: {np.array(pred)[:, n][:i].mean()}')\n",
    "# Add a label\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "predictions = stats.mode(threshold_Rounder(np.array([model.predict(test) for model in saved_models['trial']['model_list']]), qwk_thresholded).astype(np.int32))[0]\n",
    "\n",
    "submission_df = pd.read_csv('src/data/sample_submission.csv')\n",
    "\n",
    "submission_df['sii'] = predictions\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "pd.read_csv('./submission.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
